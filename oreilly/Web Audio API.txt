<div class="colophon" id="colophon">
<h1 class="title" id="colophon">Colophon</h1>
<p id="the_animal_on_t">The animal on the cover of <span class="emphasis"><em>Web Audio API </em></span>is a
  brown long-eared bat (<span class="emphasis"><em>Plecotus auritus</em></span>).</p>
<p id="the_cover_image">The cover image is from Cassell’s <span class="emphasis"><em>Natural
  History</em></span>. The cover font is Adobe ITC Garamond. The text font is
  Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code
  font is Dalton Maag's Ubuntu Mono.</p>
</div>
<section class="preface" data-original-filename="ch00.xml" id="introduction"><div class="titlepage"><div><div><h1 class="title">Preface</h1></div></div></div>
<p id="thank_you_for_p">Thank you for picking up the first book on the topic of the Web Audio
  API. When I first learned of the Web Audio API, I was a complete
  digital-audio novice embarking on a journey to learn and understand the API,
  as well as the underlying fundamental audio concepts. This book is what I
  wish existed when I started experimenting with the API in 2011. It is
  intended to be a springboard for web developers like I was, with little to
  no digital-audio expertise. It contains the things I learned from about a
  year of studying digital audio processing, having conversations with audio
  experts, and experimenting with the API.</p>
<p id="the_theoretical">The theoretical bits will be filled in through asides, which will
  explain the concepts. If you are a digital-audio guru, feel free to skip
  these. The practical bits will be illustrated with code snippets to give you
  a better sense of how the API works in real life. Many of the examples also
  include links to working samples that can be found <a class="ulink" href="http://webaudioapi.com/samples" target="_top">on this Web Audio API
  site</a>.</p>
<div class="sect1" data-original-filename="ch00.xml" id="s00_3">
<div class="titlepage"><div><div><h2 class="title">Structure of This Book</h2></div></div></div>
<p id="this_book_aims_">This book aims to give a high-level overview of a number of
    important features of the Web Audio API, but is not an exhaustive survey
    of every available feature. It is not intended as a comprehensive guide,
    but as an easy starting point. Most sections of the book start off by
    describing an application, outlining the problem and solution, and then
    showing relevant sample JavaScript Web Audio API code. Interspersed theory
    sections explain some of the underlying audio concepts in more general
    terms. The book is structured in the following way:</p>
<div class="orderedlist" id="covers_the_basi_id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="covers_the_basi_id2"><a class="xref" href="ch01.html" title="Chapter 1. Fundamentals">Chapter 1</a> covers the
        basics of audio graphs, typical graph configurations, audio nodes
        inside those graphs, loading sound files, and playing sounds
        back.</p></li>
<li class="listitem"><p id="delves_into_pre"><a class="xref" href="ch02.html" title="Chapter 2. Perfect Timing and Latency">Chapter 2</a> delves into
        precise scheduling of sound in the future, multiple simultaneous
        sounds, changing parameters directly or over time, and
        crossfading.</p></li>
<li class="listitem"><p id="covers_gain_vo"><a class="xref" href="ch03.html" title="Chapter 3. Volume and Loudness">Chapter 3</a> covers gain,
        volume, and loudness, as well as preventing clipping via metering and
        dynamics compression.</p></li>
<li class="listitem"><p id="is_all_about_so"><a class="xref" href="ch04.html" title="Chapter 4. Pitch and the Frequency Domain">Chapter 4</a> is all about
        sound frequency, an important property of periodic sound. We’ll also
        talk about oscillators and examining sound in the frequency <span class="keep-together">domain</span>.</p></li>
<li class="listitem"><p id="builds_on_the_e"><a class="xref" href="ch05.html" title="Chapter 5. Analysis and Visualization">Chapter 5</a> builds on the
        earlier chapters to dive into more advanced topics, including biquad
        filters, simulating acoustic environments, and spatialized
        sounds.</p></li>
<li class="listitem"><p id="in__we_will_ta">In <a class="xref" href="ch06.html" title="Chapter 6. Advanced Topics">Chapter 6</a>, we will
        take a break from synthesizing and manipulating sound, and analyze and
        visualize sound instead.</p></li>
<li class="listitem"><p id="talks_about_int"><a class="xref" href="ch07.html" title="Chapter 7. Integrating with Other Technologies">Chapter 7</a> talks about
        interfacing Web Audio API with other web APIs like WebRTC and the
        <code class="literal">&lt;audio&gt;</code> tag.</p></li>
</ol></div>
<p id="the_source_code">The source code of the book itself is released under the Creative
    Commons license and is available on <a class="ulink" href="https://github.com/borismus/webaudioapi.com" target="_top">GitHub</a>.</p>
</div>
<div class="sect1" data-original-filename="ch00.xml" id="conventions_used_in_this_book">
<div class="titlepage"><div><div><h2 class="title">Conventions Used in This Book</h2></div></div></div>
<p id="the_following_t">The following typographical conventions are used in this
    book:</p>
<div class="variablelist" id="italicindicates"><dl class="variablelist">
<dt><span class="term"><span class="emphasis"><em>Italic</em></span></span></dt>
<dd><p id="indicates_new_t">Indicates new terms, URLs, email addresses, filenames, and
          file extensions.</p></dd>
<dt><span class="term"><code class="literal">Constant width</code></span></dt>
<dd><p id="used_for_progra">Used for program listings, as well as within paragraphs to
          refer to program elements such as variable or function names,
          databases, data types, environment variables, statements, and
          keywords.</p></dd>
<dt><span class="term"><strong class="userinput"><code>Constant width
        bold</code></strong></span></dt>
<dd><p id="shows_commands_">Shows commands or other text that should be typed literally by
          the user.</p></dd>
<dt><span class="term"><em class="replaceable"><code>Constant width italic</code></em></span></dt>
<dd><p id="shows_text_that">Shows text that should be replaced with user-supplied values
          or by values determined by context.</p></dd>
</dl></div>
<div class="tip" id="this_icon_signi_id1"><p id="this_icon_signi_id2">This icon signifies a tip, suggestion, or general note.</p></div>
<div class="caution" id="this_icon_indic_id1"><p id="this_icon_indic_id2">This icon indicates a warning or caution.</p></div>
</div>
<div class="sect1" data-original-filename="ch00.xml" id="using_code_examples">
<div class="titlepage"><div><div><h2 class="title">Using Code Examples</h2></div></div></div>
<p id="this_book_is_he">This book is here to help you get your job done. In general, if this
    book includes code examples, you may use the code in your programs and
    documentation. You do not need to contact us for permission unless you’re
    reproducing a significant portion of the code. For example, writing a
    program that uses several chunks of code from this book does not require
    permission. Selling or distributing a CD-ROM of examples from O’Reilly
    books does require permission. Answering a question by citing this book
    and quoting example code does not require permission. Incorporating a
    significant amount of example code from this book into your product’s
    documentation does require permission.</p>
<p id="we_appreciate_">We appreciate, but do not require, attribution. An attribution
    usually includes the title, author, publisher, and ISBN. For example:
    “<span class="emphasis"><em>Web Audio API</em></span> by Boris Smus (O’Reilly). Copyright
    2013 Boris Smus, 978-1-449-33268-6.”</p>
<p id="if_you_feel_you">If you feel your use of code examples falls outside fair use or the
    permission given above, feel free to contact us at
    <code class="email">&lt;<a class="email" href="mailto:permissions@oreilly.com">permissions@oreilly.com</a>&gt;</code>.</p>
</div>
<div class="sect1" data-original-filename="ch00.xml" id="safari_books_online">
<div class="titlepage"><div><div><h2 class="title">Safari® Books Online</h2></div></div></div>
<div class="note safaribooksonline" id="safari_books_on_id1"><p id="safari_books_on_id2">Safari Books Online (<a class="ulink" href="http://my.safaribooksonline.com/?portal=oreilly" target="_top">www.safaribooksonline.com</a>)
      is an on-demand digital library that delivers expert <a class="ulink" href="http://www.safaribooksonline.com/content" target="_top">content</a> in both
      book and video form from the world’s leading authors in technology and
      business.</p></div>
<p id="technology_prof">Technology professionals, software developers, web designers, and
    business and creative professionals use Safari Books Online as their
    primary resource for research, problem solving, learning, and
    certification training.</p>
<p id="safari_books_on_id3">Safari Books Online offers a range of <a class="ulink" href="http://www.safaribooksonline.com/subscriptions" target="_top">product mixes</a>
    and pricing programs for <a class="ulink" href="http://www.safaribooksonline.com/organizations-teams" target="_top">organizations</a>,
    <a class="ulink" href="http://www.safaribooksonline.com/government" target="_top">government
    agencies</a>, and <a class="ulink" href="http://www.safaribooksonline.com/individuals" target="_top">individuals</a>.
    Subscribers have access to thousands of books, training videos, and
    prepublication manuscripts in one fully searchable database from
    publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley
    Professional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press,
    Cisco Press, John Wiley &amp; Sons, Syngress, Morgan Kaufmann, IBM
    Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,
    McGraw-Hill, Jones &amp; Bartlett, Course Technology, and dozens <a class="ulink" href="http://www.safaribooksonline.com/publishers" target="_top">more</a>. For more
    information about Safari Books Online, please visit us <a class="ulink" href="http://www.safaribooksonline.com/" target="_top">online</a>.</p>
</div>
<div class="sect1" data-original-filename="ch00.xml" id="how_to_contact_us">
<div class="titlepage"><div><div><h2 class="title">How to Contact Us</h2></div></div></div>
<p id="please_address_">Please address comments and questions concerning this book to the
    publisher:</p>
<table style="border: 0; " class="simplelist">
<tr><td>O’Reilly Media, Inc.</td></tr>
<tr><td>1005 Gravenstein Highway North</td></tr>
<tr><td>Sebastopol, CA 95472</td></tr>
<tr><td>800-998-9938 (in the United States or Canada)</td></tr>
<tr><td>707-829-0515 (international or local)</td></tr>
<tr><td>707-829-0104 (fax)</td></tr>
</table>
<p id="we_have_a_web_p">We have a web page for this book, where we list errata, examples,
    and any additional information. You can access this page at <a class="ulink" href="http://oreil.ly/web-audio-api" target="_top">http://oreil.ly/web-audio-api</a>.</p>
<p id="to_comment_or_a">To comment or ask technical questions about this book, send email to
    <code class="email">&lt;<a class="email" href="mailto:bookquestions@oreilly.com">bookquestions@oreilly.com</a>&gt;</code>.</p>
<p id="for_more_inform_id1">For more information about our books, courses, conferences, and
    news, see our website at <a class="ulink" href="http://www.oreilly.com" target="_top">http://www.oreilly.com</a>.</p>
<p id="find_us_on_face">Find us on Facebook: <a class="ulink" href="http://facebook.com/oreilly" target="_top">http://facebook.com/oreilly</a></p>
<p id="follow_us_on_tw">Follow us on Twitter: <a class="ulink" href="http://twitter.com/oreillymedia" target="_top">http://twitter.com/oreillymedia</a></p>
<p id="watch_us_on_you">Watch us on YouTube: <a class="ulink" href="http://www.youtube.com/oreillymedia" target="_top">http://www.youtube.com/oreillymedia</a></p>
</div>
<div class="sect1" data-original-filename="ch00.xml" id="s00_4">
<div class="titlepage"><div><div><h2 class="title">Thanks!</h2></div></div></div>
<p id="i_am_not_an_exp">I am not an expert in digital signals processing, mastering, or
    mixing by any stretch. I am a software engineer and amateur musician with
    enough interest in digital audio to spend some time exploring the Web
    Audio API and wrapping my head around some of its important concepts. To
    write this book, I had to continually bug others with far more
    digital-audio experience than me. I’d like to thank them for answering my
    questions, providing technical reviews for this book, and encouraging me
    along the way.</p>
<p id="specifically_t">Specifically, this book could not have been written without the
    generous mentorship of Chris Rogers, the primary author of the Web Audio
    specification and also its main WebKit/Chrome implementer. I owe many
    thanks to Chris Wilson, who gave an incredibly thorough technical review
    of this book’s content, and to Mark Goldstein, who spent a few late nights
    doing editorial passes. My thanks to Kevin Ennis for donating <a class="ulink" href="http://webaudioapi.com" target="_top">webaudioapi.com</a> for hosting samples
    related to the book. Last but not least, I would have never written this
    book without the support and interest of a vibrant Web Audio API community
    on the Web.</p>
<p id="without_further">Without further ado, let’s dive in!</p>
</div></section><section class="chapter" data-original-filename="ch01.xml" id="ch01"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Fundamentals</h1></div></div></div>
<p id="this_chapter_wi">This chapter will describe how to get started with the Web Audio API,
  which browsers are supported, how to detect if the API is available, what an
  audio graph is, what audio nodes are, how to connect nodes together, some
  basic node types, and finally, how to load sound files and playback
  sounds.</p>
<div class="sect1" data-original-filename="ch01.xml" id="s00_1">
<div class="titlepage"><div><div><h2 class="title">A Brief History of Audio on the Web</h2></div></div></div>
<p id="the_first_way_o">The first way of playing back sounds on the web was via the <code class="literal">&lt;bgsound&gt;</code> tag, which let website authors
    automatically play background music when a visitor opened their pages.
    This feature was only available in Internet Explorer, and was never
    standardized or picked up by other browsers. Netscape implemented a
    similar feature with the <span class="keep-together"><code class="literal">&lt;embed&gt;</code></span> tag, providing basically
    equivalent functionality.</p>
<p id="flash_was_the_f">Flash was the first cross-browser way of playing back audio on the
    Web, but it had the significant drawback of requiring a plug-in to run.
    More recently, browser vendors have rallied around the HTML5 <code class="literal">&lt;audio&gt;</code> element, which provides native
    support for audio playback in all modern browsers.</p>
<p id="although_audio_">Although audio on the Web no longer requires a plug-in, the <code class="literal">&lt;audio&gt;</code> tag has significant limitations
    for implementing sophisticated games and interactive applications. The
    following are just some of the limitations of the <code class="literal">&lt;audio&gt;</code> element:</p>
<div class="itemizedlist" id="no_precise_timi_id1"><ul class="itemizedlist">
<li class="listitem"><p id="no_precise_timi_id2">No precise timing controls</p></li>
<li class="listitem"><p id="very_low_limit_">Very low limit for the number of sounds played at once</p></li>
<li class="listitem"><p id="no_way_to_relia">No way to reliably pre-buffer a sound</p></li>
<li class="listitem"><p id="no_ability_to_a">No ability to apply real-time effects</p></li>
<li class="listitem"><p id="no_way_to_analy">No way to analyze sounds</p></li>
</ul></div>
<p id="there_have_been">There have been several attempts to create a powerful audio API on
    the Web to address some of the limitations I previously described. One
    notable example is the Audio Data API that was designed and prototyped in
    Mozilla Firefox. Mozilla’s approach started with an <code class="literal">&lt;audio&gt;</code> element and extended its
    JavaScript API with additional features. This API has a limited audio
    graph (more on this later in <a class="xref" href="ch01.html#s01_1" title="The Audio Context">“The Audio Context”</a>), and hasn’t been
    adopted beyond its first implementation. It is currently deprecated in
    Firefox in favor of the Web Audio API.</p>
<p id="in_contrast_wit">In contrast with the Audio Data API, the Web Audio API is a brand
    new model, completely separate from the <code class="literal">&lt;audio&gt;</code> tag, although there are
    integration points with other web APIs (see <a class="xref" href="ch07.html" title="Chapter 7. Integrating with Other Technologies">Chapter 7</a>). It
    is a high-level JavaScript API for processing and synthesizing audio in
    web applications. The goal of this API is to include capabilities found in
    modern game engines and some of the mixing, processing, and filtering
    tasks that are found in modern desktop audio production applications. The
    result is a versatile API that can be used in a variety of audio-related
    tasks, from games, to interactive applications, to very advanced music
    synthesis applications and visualizations.</p>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s00_2">
<div class="titlepage"><div><div><h2 class="title">Games and Interactivity</h2></div></div></div>
<p id="audio_is_a_huge">Audio is a huge part of what makes interactive experiences so
    compelling. If you don’t believe me, try watching a movie with the volume
    muted.</p>
<p id="games_are_no_ex">Games are no exception! My fondest video game memories are of the
    music and sound effects. Now, nearly two decades after the release of some
    of my favorites, I still can’t get Koji Kondo’s <span class="emphasis"><em>Zelda</em></span>
    and Matt Uelmen’s <span class="emphasis"><em>Diablo</em></span> soundtracks out of my head.
    Even the sound effects from these masterfully-designed games are instantly
    recognizable, from the unit click responses in Blizzard’s
    <span class="emphasis"><em>Warcraft</em></span> and <span class="emphasis"><em>Starcraft</em></span> series to
    samples from Nintendo’s classics.</p>
<p id="sound_effects_m">Sound effects matter a great deal outside of games, too. They have
    been around in user interfaces (UIs) since the days of the command line,
    where certain kinds of errors would result in an audible beep. The same
    idea continues through modern UIs, where well-placed sounds are critical
    for notifications, chimes, and of course audio and video communication
    applications like Skype. Assistant software such as Google Now and Siri
    provide rich, audio-based feedback. As we delve further into a world of
    ubiquitous computing, speech- and gesture-based interfaces that lend
    themselves to screen-free interactions are increasingly reliant on audio
    feedback. Finally, for visually impaired computer users, audio cues,
    speech synthesis, and speech recognition are critically important to
    create a usable experience.</p>
<p id="interactive_aud">Interactive audio presents some interesting challenges. To create
    convincing in-game music, designers need to adjust to all the potentially
    unpredictable game states a player can find herself in. In practice,
    sections of the game can go on for an unknown duration, and sounds can
    interact with the environment and mix in complex ways, requiring
    environment-specific effects and relative sound positioning. Finally,
    there can be a large number of sounds playing at once, all of which need
    to sound good together and render without introducing quality and
    performance penalties.</p>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_1">
<div class="titlepage"><div><div><h2 class="title">The Audio Context</h2></div></div></div>
<p id="the_web_audio_a_id1">The Web Audio API is built around the concept of an audio context.
    The audio context is a directed graph of audio nodes that defines how the
    audio stream flows from its source (often an audio file) to its
    destination (often your speakers). As audio passes through each node, its
    properties can be modified or inspected. The simplest audio context is a
    connection directly form a source node to a destination node (<a class="xref" href="ch01.html#fig1" title="Figure 1-1. The simplest audio context">Figure 1-1</a>).</p>
<div class="figure" id="fig1">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0101.png" alt="The simplest audio context"></div></div>
<div class="figure-title">Figure 1-1. The simplest audio context</div>
</div>
<p id="an_audio_contex">An audio context can be complex, containing many nodes between the
    source and destination (<a class="xref" href="ch01.html#fig2" title="Figure 1-2. A more complex audio context">Figure 1-2</a>) to perform arbitrarily
    advanced synthesis or analysis.</p>
<p id="figures_and_sho">Figures <a class="xref" href="ch01.html#fig1" title="Figure 1-1. The simplest audio context">1-1</a> and
    <a class="xref" href="ch01.html#fig2" title="Figure 1-2. A more complex audio context">1-2</a> show audio nodes
    as blocks. The arrows represent connections between nodes. Nodes can often
    have multiple incoming and outgoing connections. By default, if there are
    multiple incoming connections into a node, the Web Audio API simply blends
    the incoming audio signals together.</p>
<p id="the_concept_of_">The concept of an audio node graph is not new, and derives from
    popular audio frameworks such as Apple’s CoreAudio, which has an analogous
    <a class="ulink" href="http://bit.ly/15tPRNM" target="_top">Audio Processing Graph API</a>. The
    idea itself is even older, originating in the 1960s with early audio
    environments like <a class="ulink" href="http://en.wikipedia.org/wiki/Moog_synthesizer" target="_top">Moog modular
    synthesizer systems</a>.</p>
<div class="figure" id="fig2">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0102.png" alt="A more complex audio context"></div></div>
<div class="figure-title">Figure 1-2. A more complex audio context</div>
</div>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_2">
<div class="titlepage"><div><div><h2 class="title">Initializing an Audio Context</h2></div></div></div>
<p id="the_web_audio_a_id2">The Web Audio API is currently implemented by the Chrome and Safari
    browsers (including <span class="keep-together">Mobile</span>Safari as
    of iOS 6) and is available for web developers via JavaScript. In these
    browsers, the audio context constructor is webkit-prefixed, meaning that
    instead of creating a new <code class="literal">AudioContext</code>,
    you create a new <code class="literal">webkitAudioContext</code>.
    However, this will surely change in the future as the API stabilizes
    enough to ship un-prefixed and as other browser vendors implement it.
    Mozilla has <a class="ulink" href="https://wiki.mozilla.org/Web_Audio_API" target="_top">publicly
    stated</a> that they are implementing the Web Audio API in Firefox,
    and Opera has <a class="ulink" href="http://lists.w3.org/Archives/Public/public-audio/2012AprJun/0279.html" target="_top">started
    participating</a> in the working group.</p>
<p id="with_this_in_mi">With this in mind, here is a liberal way of initializing your audio
    context that would include other implementations (once they exist):</p>
<pre class="programlisting" data-language="javascript" id="var_contextclas"><code class="kd">var</code> <code class="nx">contextClass</code> <code class="o">=</code> <code class="p">(</code><code class="nb">window</code><code class="p">.</code><code class="nx">AudioContext</code> <code class="o">||</code> 
  <code class="nb">window</code><code class="p">.</code><code class="nx">webkitAudioContext</code> <code class="o">||</code> 
  <code class="nb">window</code><code class="p">.</code><code class="nx">mozAudioContext</code> <code class="o">||</code> 
  <code class="nb">window</code><code class="p">.</code><code class="nx">oAudioContext</code> <code class="o">||</code> 
  <code class="nb">window</code><code class="p">.</code><code class="nx">msAudioContext</code><code class="p">);</code>
<code class="k">if</code> <code class="p">(</code><code class="nx">contextClass</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Web Audio API is available.</code>
  <code class="kd">var</code> <code class="nx">context</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">contextClass</code><code class="p">();</code>
<code class="p">}</code> <code class="k">else</code> <code class="p">{</code>
  <code class="c1">// Web Audio API is not available. Ask the user to use a supported browser.</code>
<code class="p">}</code></pre>
<p id="a_single_audio_">A single audio context can support multiple sound inputs and complex
    audio graphs, so generally speaking, we will only need one for each audio
    application we create. The audio context instance includes many methods
    for creating audio nodes and manipulating global audio preferences.
    Luckily, these methods are not webkit-prefixed and are relatively stable.
    The API is still changing, though, so be aware of breaking changes (see
    <a class="xref" href="apa.html" title="Appendix A. Deprecation Notes">Appendix A</a>).</p>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_3">
<div class="titlepage"><div><div><h2 class="title">Types of Web Audio Nodes</h2></div></div></div>
<p id="one_of_the_main">One of the main uses of audio contexts is to create new audio nodes.
    Broadly speaking, there are several kinds of audio nodes:</p>
<div class="variablelist" id="source_nodessou"><dl class="variablelist">
<dt><span class="term">Source nodes</span></dt>
<dd><p id="sound_sources_s">Sound sources such as audio buffers, live audio inputs,
          <code class="literal">&lt;audio&gt;</code> tags, oscillators,
          and JS processors</p></dd>
<dt><span class="term">Modification nodes</span></dt>
<dd><p id="filters_convol">Filters, convolvers, panners, JS processors, etc.</p></dd>
<dt><span class="term">Analysis nodes</span></dt>
<dd><p id="analyzers_and_j">Analyzers and JS processors</p></dd>
<dt><span class="term">Destination nodes</span></dt>
<dd><p id="audio_outputs_a">Audio outputs and offline processing buffers</p></dd>
</dl></div>
<p id="sources_need_no">Sources need not be based on sound files, but can instead be
    real-time input from a live instrument or microphone, redirection of the
    audio output from an audio element [see <a class="xref" href="ch07.html#s07_1" title="Setting Up Background Music with the &lt;audio&gt; Tag">“Setting Up Background Music with the &lt;audio&gt; Tag”</a>], or
    entirely synthesized sound [see <a class="xref" href="ch06.html#s06_6" title="Audio Processing with JavaScript">“Audio Processing with JavaScript”</a>]. Though the
    final destination-node is often the speakers, you can also process without
    sound playback (for example, if you want to do pure visualization) or do
    offline processing, which results in the audio stream being written to a
    destination buffer for later use.</p>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_4">
<div class="titlepage"><div><div><h2 class="title">Connecting the Audio Graph</h2></div></div></div>
<p id="any_audio_node">Any audio node’s output can be connected to any other audio node’s
    input by using the <code class="literal">connect()</code> function.
    In the following example, we connect a source node’s output into a gain
    node, and connect the gain node’s output into the context’s
    destination:</p>
<pre class="programlisting" data-language="javascript" id="create_the_s"><code class="c1">// Create the source.</code>
<code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
<code class="c1">// Create the gain node.</code>
<code class="kd">var</code> <code class="nx">gain</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createGain</code><code class="p">();</code>
<code class="c1">// Connect source to filter, filter to destination.</code>
<code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">gain</code><code class="p">);</code>
<code class="nx">gain</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code></pre>
<p id="note_that_conte">Note that <code class="code">context.destination</code> is a special node that is
    associated with the default audio output of your system. The resulting
    audio graph of the previous code looks like <a class="xref" href="ch01.html#fig3" title="Figure 1-3. Our first audio graph">Figure 1-3</a>.</p>
<div class="figure" id="fig3">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0103.png" alt="Our first audio graph"></div></div>
<div class="figure-title">Figure 1-3. Our first audio graph</div>
</div>
<p id="once_we_have_co">Once we have connected up a graph like this we can dynamically
    change it. We can disconnect audio nodes from the graph by calling
    <code class="code">node.disconnect(outputNumber)</code>. For example, to reroute a
    direct connection between source and destination, circumventing the
    intermediate node, we can do the following:</p>
<pre class="programlisting" data-language="javascript" id="sourcedisconne"><code class="nx">source</code><code class="p">.</code><code class="nx">disconnect</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="nx">gain</code><code class="p">.</code><code class="nx">disconnect</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code></pre>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_5">
<div class="titlepage"><div><div><h2 class="title">Power of Modular Routing</h2></div></div></div>
<p id="in_many_games_">In many games, multiple sources of sound are combined to create the
    final mix. Sources include background music, game sound effects, UI
    feedback sounds, and in a multiplayer setting, voice chat from other
    players. An important feature of the Web Audio API is that it lets you
    separate all of these different channels and gives you full control over
    each one, or all of them together. The audio graph for such a setup might
    look like <a class="xref" href="ch01.html#fig4" title="Figure 1-4. Multiple sources with individual gain control as well as a master gain">Figure 1-4</a>.</p>
<div class="figure" id="fig4">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0104.png" alt="Multiple sources with individual gain control as well as a master gain"></div></div>
<div class="figure-title">Figure 1-4. Multiple sources with individual gain control as well as a master
      gain</div>
</div>
<p id="we_have_associa">We have associated a separate gain node with each of the channels
    and also created a master gain node to control them all. With this setup,
    it is easy for your players to control the level of each channel
    separately, precisely the way they want to. For example, many people
    prefer to play games with the background music turned off.</p>
<div class="sidebar theory" id="s01_6">
<div class="titlepage"><div><div><div class="sidebar-title">What Is Sound?</div></div></div></div>
<p id="in_terms_of_phy">In terms of physics, sound is a longitudinal wave (sometimes
      called a pressure wave) that travels through air or another medium. The
      source of the sound causes molecules in the air to vibrate and collide
      with one another. This causes regions of high and low pressure, which
      come together and fall apart in bands. If you could freeze time and look
      at the pattern of a sound wave, you might see something like <a class="xref" href="ch01.html#fig5" title="Figure 1-5. A sound pressure wave traveling through air particles">Figure 1-5</a>.</p>
<div class="figure" id="fig5">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0105.png" alt="A sound pressure wave traveling through air particles"></div></div>
<div class="figure-title">Figure 1-5. A sound pressure wave traveling through air particles</div>
</div>
<p id="mathematically">Mathematically, sound can be represented as a function, which
      ranges over pressure values across the domain of time. <a class="xref" href="ch01.html#fig6" title="Figure 1-6. A mathematical representation of the sound wave in Figure 1-5">Figure 1-6</a> shows a graph of such a function. You can see that it
      is analogous to <a class="xref" href="ch01.html#fig5" title="Figure 1-5. A sound pressure wave traveling through air particles">Figure 1-5</a>, with high values corresponding
      to areas with dense particles (high pressure), and low values
      corresponding to areas with sparse particles (low pressure).</p>
<div class="figure" id="fig6">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0106.png" alt="A mathematical representation of the sound wave in"></div></div>
<div class="figure-title">Figure 1-6. A mathematical representation of the sound wave in <a class="xref" href="ch01.html#fig5" title="Figure 1-5. A sound pressure wave traveling through air particles">Figure 1-5</a>
</div>
</div>
<p id="electronics_dat">Electronics dating back to the early twentieth century made it
      possible for us to capture and recreate sounds for the first time.
      Microphones take the pressure wave and convert it into an electric
      signal, where (for example) +5 volts corresponds to the highest pressure
      and −5 volts to the lowest. Conversely, audio speakers take this voltage
      and convert it back into the pressure waves that we can hear.</p>
<p id="whether_we_are_">Whether we are analyzing sound or synthesizing it, the interesting
      bits for audio programmers are in the black box in <a class="xref" href="ch01.html#fig7" title="Figure 1-7. Recording and playback">Figure 1-7</a>, tasked with manipulating the audio signal. In the
      early days of audio, this space was occupied by analog filters and other
      hardware that would be considered archaic by today’s standards. Today,
      there are modern digital equivalents for many of those old analog pieces
      of equipment. But before we can use software to tackle the fun stuff, we
      need to represent sound in a way that computers can work with.</p>
<div class="figure" id="fig7">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0107.png" alt="Recording and playback"></div></div>
<div class="figure-title">Figure 1-7. Recording and playback</div>
</div>
</div>
<div class="sidebar theory" id="what_is_digital_sound">
<div class="titlepage"><div><div><div class="sidebar-title">What Is Digital Sound?</div></div></div></div>
<p id="we_can_do_this_">We can do this by time-sampling the analog signal at some
      frequency, and encoding the signal at each sample as a number. The rate
      at which we sample the analog signal is called the <span class="emphasis"><em>sample
      rate</em></span>. A common sample rate in many sound applications is 44.1
      kHz. This means that there are 44,100 numbers recorded for each second
      of sound. The numbers themselves must fall within some range. There is
      generally a certain number of bits allocated to each value, which is
      called the <span class="emphasis"><em>bit depth</em></span>. For most recorded digital
      audio, including CDs, the bit depth is 16, which is generally enough for
      most listeners. Audiophiles prefer 24-bit depth, which gives enough
      precision that people’s ears can’t hear the difference compared to a
      higher depth.</p>
<p id="the_process_of_">The process of converting analog signals into digital ones is
      called <span class="emphasis"><em>quantization</em></span> (or sampling) and is
      illustrated in <a class="xref" href="ch01.html#fig8" title="Figure 1-8. Analog sound being quantized, or transformed into digital sound">Figure 1-8</a>.</p>
<div class="figure" id="fig8">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0108.png" alt="Analog sound being quantized, or transformed into digital sound"></div></div>
<div class="figure-title">Figure 1-8. Analog sound being quantized, or transformed into digital
        sound</div>
</div>
<p id="in__the_quanti">In <a class="xref" href="ch01.html#fig8" title="Figure 1-8. Analog sound being quantized, or transformed into digital sound">Figure 1-8</a>, the quantized digital version isn’t
      quite the same as the analog one because of differences between the bars
      and the smooth line. The difference (shown in blue) decreases with
      higher sample rates and bit depths. However, increasing these values
      also increases the amount of storage required to keep these sounds in
      memory, on disk, or on the Web. To save space, telephone systems often
      used sample rates as low as 8 kHz, since the range of frequencies needed
      to make the human voice intelligible is far smaller than our full
      audible-frequency range.</p>
<p id="by_digitizing_s">By digitizing sound, computers can treat sounds like long arrays
      of numbers. This sort of encoding is called <span class="emphasis"><em>pulse-code
      modulation (PCM)</em></span>. Because computers are so good at processing
      arrays, PCM turns out to be a very powerful primitive for most
      digital-audio applications. In the Web Audio API world, this long array
      of numbers representing a sound is abstracted as an
      <code class="literal">AudioBuffer</code>. <code class="literal">AudioBuffer</code>s can
      store multiple audio channels (often in stereo, meaning a left and right
      channel) represented as arrays of floating-point numbers normalized
      between −1 and 1. The same signal can also be represented as an array of
      integers, which, in 16-bit, range from (−2<sup>15</sup>)
      to (2<sup>15</sup> − 1).</p>
</div>
<div class="sidebar theory" id="s01_7">
<div class="titlepage"><div><div><div class="sidebar-title">Audio Encoding Formats</div></div></div></div>
<p id="raw_audio_in_pc">Raw audio in PCM format is quite large, which uses extra memory,
      wastes space on a hard drive, and takes up extra bandwidth when
      downloaded. Because of this, audio is generally stored in compressed
      formats. There are two kinds of compression: lossy and lossless.
      Lossless compression (e.g., FLAC) guarantees that when you compress and
      then uncompress a sound, the bits are identical. Lossy audio compression
      (e.g., MP3) exploits features of human hearing to save additional space
      by throwing out bits that we probably won’t be able to hear anyway.
      Lossy formats are generally good enough for most people, with the
      exception of some audiophiles.</p>
<p id="a_commonly_used">A commonly used metric for the amount of compression in audio is
      called <span class="emphasis"><em>bit rate</em></span>, which refers to the number of bits
      of audio required per second of playback. The higher the bit rate, the
      more data that can be allocated per unit of time, and thus the less
      compression required. Often, lossy formats, such as MP3, are described
      by their bit rate (common rates are 128 and 192 Kb/s). It’s possible to
      encode lossy codecs at arbitrary bit rates. For example,
      telephone-quality human speech is often compared to 8 Kb/s MP3s. Some
      formats such as OGG support variable bit rates, where the bit rate
      changes over time. Be careful not to confuse this concept with sample
      rate or bit depth [see <a class="xref" href="ch01.html#s01_6" title="What Is Sound?">“What Is Sound?”</a>]!</p>
<p id="browser_support">Browser support for different audio formats varies quite a bit.
      Generally, if the Web Audio API is implemented in a browser, it uses the
      same loading code that the <code class="literal">&lt;audio&gt;</code> tag would, so the browser
      support matrix for <code class="literal">&lt;audio&gt;</code> and
      the Web Audio API is the same. Generally, WAV (which is a simple,
      lossless, and typically uncompressed format) is supported in all
      browsers. MP3 is still patent-encumbered, and is therefore not available
      in some purely open source browsers (e.g., Firefox and Chromium).
      Unfortunately, the less popular but patent-unencumbered OGG format is
      still not available in Safari at the time of this writing.</p>
<p id="for_a_more_upt">For a more up-to-date roster of audio format support, see <a class="ulink" href="http://mzl.la/13kGelS" target="_top">http://mzl.la/13kGelS</a>.</p>
</div>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_8">
<div class="titlepage"><div><div><h2 class="title">Loading and Playing Sounds</h2></div></div></div>
<p id="web_audio_api_m">Web Audio API makes a clear distinction between buffers and source
    nodes. The idea of this architecture is to decouple the audio asset from
    the playback state. Taking a record player analogy, buffers are like
    records and sources are like playheads, except in the Web Audio API world,
    you can play the same record on any number of playheads simultaneously!
    Because many applications involve multiple versions of the same buffer
    playing simultaneously, this pattern is essential. For example, if you
    want multiple bouncing ball sounds to fire in quick succession, you need
    to load the bounce buffer only once and schedule multiple sources of
    playback [see <a class="xref" href="ch04.html#s04_3" title="Multiple Sounds with Variations">“Multiple Sounds with Variations”</a>].</p>
<p id="to_load_an_audi">To load an audio sample into the Web Audio API, we can use an
    <code class="code">XMLHttpRequest</code> and process the results with
    <code class="code">context.decodeAudioData</code>. This all happens asynchronously and
    doesn’t block the main UI thread:</p>
<pre class="programlisting" data-language="javascript" id="var_request__n"><code class="kd">var</code> <code class="nx">request</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">XMLHttpRequest</code><code class="p">();</code>
<code class="nx">request</code><code class="p">.</code><code class="nx">open</code><code class="p">(</code><code class="s1">'GET'</code><code class="p">,</code> <code class="nx">url</code><code class="p">,</code> <code class="kc">true</code><code class="p">);</code>
<code class="nx">request</code><code class="p">.</code><code class="nx">responseType</code> <code class="o">=</code> <code class="s1">'arraybuffer'</code><code class="p">;</code>

<code class="c1">// Decode asynchronously</code>
<code class="nx">request</code><code class="p">.</code><code class="nx">onload</code> <code class="o">=</code> <code class="kd">function</code><code class="p">()</code> <code class="p">{</code>
  <code class="nx">context</code><code class="p">.</code><code class="nx">decodeAudioData</code><code class="p">(</code><code class="nx">request</code><code class="p">.</code><code class="nx">response</code><code class="p">,</code> <code class="kd">function</code><code class="p">(</code><code class="nx">theBuffer</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">buffer</code> <code class="o">=</code> <code class="nx">theBuffer</code><code class="p">;</code>
  <code class="p">},</code> <code class="nx">onError</code><code class="p">);</code>
<code class="p">}</code>
<code class="nx">request</code><code class="p">.</code><code class="nx">send</code><code class="p">();</code></pre>
<p id="audio_buffers_a">Audio buffers are only one possible source of playback. Other
    sources include direct input from a microphone or line-in device or an
    <code class="literal">&lt;audio&gt;</code> tag among others (see
    <a class="xref" href="ch07.html" title="Chapter 7. Integrating with Other Technologies">Chapter 7</a>).</p>
<p id="once_youve_loa">Once you’ve loaded your buffer, you can create a source node
    (<code class="code">AudioBufferSourceNode</code>) for it, connect the source node into
    your audio graph, and call <code class="code">start(0)</code> on the source node. To
    stop a sound, call <code class="code">stop(0)</code> on the source node. Note that both
    of these function calls require a time in the coordinate system of the
    current audio context (see <a class="xref" href="ch02.html" title="Chapter 2. Perfect Timing and Latency">Chapter 2</a>):</p>
<pre class="programlisting" data-language="javascript" id="function_playso"><code class="kd">function</code> <code class="nx">playSound</code><code class="p">(</code><code class="nx">buffer</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="nx">buffer</code><code class="p">;</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="games_often_hav">Games often have background music playing in a loop. However, be
    careful about being overly repetitive with your selection: if a player is
    stuck in an area or level, and the same sample continuously plays in the
    background, it may be worthwhile to gradually fade out the track to
    prevent frustration. Another strategy is to have mixes of various
    intensity that gradually crossfade into one another depending on the game
    situation [see <a class="xref" href="ch02.html#s02_5" title="Gradually Varying Audio Parameters">“Gradually Varying Audio Parameters”</a>].</p>
</div>
<div class="sect1" data-original-filename="ch01.xml" id="s01_9">
<div class="titlepage"><div><div><h2 class="title">Putting It All Together</h2></div></div></div>
<p id="as_you_can_see_">As you can see from the previous code listings, there’s a bit of
    setup to get sounds playing in the Web Audio API. For a real game,
    consider implementing a JavaScript abstraction around the Web Audio API.
    An example of this idea is the following <code class="literal">BufferLoader</code>
    class. It puts everything together into a simple loader, which, given a
    set of paths, returns a set of audio buffers. Here’s how such a class can
    be used:</p>
<pre class="programlisting" data-language="javascript" id="windowonload_"><code class="nb">window</code><code class="p">.</code><code class="nx">onload</code> <code class="o">=</code> <code class="nx">init</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">context</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">bufferLoader</code><code class="p">;</code>

<code class="kd">function</code> <code class="nx">init</code><code class="p">()</code> <code class="p">{</code>
  <code class="nx">context</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">webkitAudioContext</code><code class="p">();</code>

  <code class="nx">bufferLoader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">BufferLoader</code><code class="p">(</code>
    <code class="nx">context</code><code class="p">,</code>
    <code class="p">[</code>
      <code class="s1">'../sounds/hyper-reality/br-jam-loop.wav'</code><code class="p">,</code>
      <code class="s1">'../sounds/hyper-reality/laughter.wav'</code><code class="p">,</code>
    <code class="p">],</code>
    <code class="nx">finishedLoading</code>
    <code class="p">);</code>

  <code class="nx">bufferLoader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>
<code class="p">}</code>

<code class="kd">function</code> <code class="nx">finishedLoading</code><code class="p">(</code><code class="nx">bufferList</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Create two sources and play them both together.</code>
  <code class="kd">var</code> <code class="nx">source1</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="kd">var</code> <code class="nx">source2</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="nx">source1</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="nx">bufferList</code><code class="p">[</code><code class="mi">0</code><code class="p">];</code>
  <code class="nx">source2</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="nx">bufferList</code><code class="p">[</code><code class="mi">1</code><code class="p">];</code>

  <code class="nx">source1</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
  <code class="nx">source2</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
  <code class="nx">source1</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="nx">source2</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="for_a_simple_re">For a simple reference implementation of <code class="code">BufferLoader</code>,
    take a look at <a class="ulink" href="http://webaudioapi.com/samples/shared.js" target="_top">http://webaudioapi.com/samples/shared.js</a>.</p>
</div></section><section class="chapter" data-original-filename="ch02.xml" id="ch02"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Perfect Timing and Latency</h1></div></div></div>
<p id="one_of_the_stre">One of the strengths of the Web Audio API as compared to the <code class="literal">&lt;audio&gt;</code> tag is that it comes with a
  low-latency precise-timing model.</p>
<p id="low_latency_is_">Low latency is very important for games and other interactive
  applications since you often need fast auditory response to user actions. If
  the feedback is not immediate, the user will sense the delay, which will
  lead to frustration. In practice, due to imperfections of human hearing,
  there is leeway for a delay of up to 20 ms or so, but the number varies
  depending on many factors.</p>
<p id="precise_timing_">Precise timing enables you to schedule events at specific times in the
  future. This is very important for scripted scenes and musical
  applications.</p>
<div class="sect1" data-original-filename="ch02.xml" id="s02_1">
<div class="titlepage"><div><div><h2 class="title">Timing Model</h2></div></div></div>
<p id="one_of_the_key_">One of the key things that the audio context provides is a
    consistent timing model and frame of reference for time. Importantly, this
    model is different from the one used for JavaScript timers such as
    <code class="code">setTimeout</code>, <code class="code">setInterval</code>, and <code class="code">new
    Date()</code>. It is also different from the performance clock provided by
    <code class="code">window.performance.now()</code>.</p>
<p id="all_of_the_abso">All of the absolute times that you will be dealing with in the Web
    Audio API are in seconds (not milliseconds!), in the coordinate system of
    the specified audio context. The current time can be retrieved from the
    audio context via the <code class="code">currentTime</code> property. Although the
    units are seconds, time is stored as a floating-point value with high
    precision.</p>
</div>
<div class="sect1" data-original-filename="ch02.xml" id="s02_2">
<div class="titlepage"><div><div><h2 class="title">Precise Playback and Resume</h2></div></div></div>
<p id="the_start_fun">The <code class="code">start()</code> function makes it easy to schedule precise
    sound playback for games and other time-critical applications. To get this
    working properly, ensure that your sound buffers are pre-loaded [see <a class="xref" href="ch01.html#s01_8" title="Loading and Playing Sounds">“Loading and Playing Sounds”</a>]. Without a <span class="keep-together">pre-loaded</span> buffer, you will have to wait an
    unknown amount of time for the browser to fetch the sound file, and then
    for the Web Audio API to decode it. The failure mode in this case is you
    want to play a sound at a precise instant, but the buffer is still loading
    or decoding.</p>
<p id="sounds_can_be_s">Sounds can be scheduled to play at a precise time by specifying the
    first (<code class="code">when</code>) parameter of the <code class="code">start()</code> call. This
    parameter is in the coordinate system of the <code class="code">AudioContext</code>’s
    <code class="code">currentTime</code>. If the parameter is less than the <code class="literal">currentTime</code>, it is played immediately. Thus
    <code class="code">start(0)</code> always plays sound immediately, but to schedule
    sound in 5 seconds, you would call <code class="code">start(context.currentTime +
    5)</code>.</p>
<p id="sound_buffers_c">Sound buffers can also be played from a specific time offset by
    specifying a second parameter to the <code class="code">start()</code> call, and
    limited to a specific duration with a third optional parameter. For
    example, if we want to pause a sound and play it back from the paused
    position, we can implement a pause by tracking the amount of time a sound
    has been playing in the current session and also tracking the last offset
    in order to resume later:</p>
<pre class="programlisting" data-language="javascript" id="assume_conte"><code class="c1">// Assume context is a web audio context, buffer is a pre-loaded audio buffer.</code>
<code class="kd">var</code> <code class="nx">startOffset</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">startTime</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code>

<code class="kd">function</code> <code class="nx">pause</code><code class="p">()</code> <code class="p">{</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">stop</code><code class="p">();</code>
  <code class="c1">// Measure how much time passed since the last pause.</code>
  <code class="nx">startOffset</code> <code class="o">+=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code> <code class="o">-</code> <code class="nx">startTime</code><code class="p">;</code>
<code class="p">}</code></pre>
<p id="once_a_source_n">Once a source node has finished playing back, it can’t play back
    more. To play back the underlying buffer again, you need to create a new
    source node (<code class="code">AudioBufferSourceNode</code>) and call <code class="literal">start()</code>:</p>
<pre class="programlisting" data-language="javascript" id="function_play"><code class="kd">function</code> <code class="nx">play</code><code class="p">()</code> <code class="p">{</code>
  <code class="nx">startTime</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="c1">// Connect graph</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="k">this</code><code class="p">.</code><code class="nx">buffer</code><code class="p">;</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">loop</code> <code class="o">=</code> <code class="kc">true</code><code class="p">;</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
  <code class="c1">// Start playback, but make sure we stay in bound of the buffer.</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nx">startOffset</code> <code class="o">%</code> <code class="nx">buffer</code><code class="p">.</code><code class="nx">duration</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="though_recreati">Though recreating the source node may seem inefficient at first,
    keep in mind that source nodes are heavily optimized for this pattern.
    Remember that if you keep a handle to the <code class="code">AudioBuffer</code>, you
    don’t need to make another request to the asset to play the same sound
    again. By having this <code class="code">AudioBuffer</code> around, you have a clean
    separation between buffer and player, and can easily play back multiple
    versions of the same buffer overlapping in time. If you find yourself
    needing to repeat this pattern, encapsulate playback with a simple helper
    function like <code class="code">playSound(buffer)</code> in an earlier code
    snippet.</p>
</div>
<div class="sect1" data-original-filename="ch02.xml" id="s02_3">
<div class="titlepage"><div><div><h2 class="title">Scheduling Precise Rhythms</h2></div></div></div>
<p id="the_web_audio_a_id3">The Web Audio API lets developers precisely schedule playback in the
    future. To demonstrate this, let’s set up a simple rhythm track. Probably
    the simplest and most widely known drumkit pattern is shown in <a class="xref" href="ch02.html#fig9" title="Figure 2-1. Sheet music for one of the most basic drum patterns">Figure 2-1</a>, in which a hihat is played every eighth note, and the
    kick and snare are played on alternating quarter notes, in 4/4
    time.</p>
<div class="figure" id="fig9">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0201.png" alt="Sheet music for one of the most basic drum patterns"></div></div>
<div class="figure-title">Figure 2-1. Sheet music for one of the most basic drum patterns</div>
</div>
<p id="assuming_we_hav">Assuming we have already loaded the kick, snare, and hihat buffers,
    the code to do this is simple:</p>
<pre class="programlisting" data-language="javascript" id="for_var_bar__"><code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">bar</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">bar</code> <code class="o">&lt;</code> <code class="mi">2</code><code class="p">;</code> <code class="nx">bar</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">time</code> <code class="o">=</code> <code class="nx">startTime</code> <code class="o">+</code> <code class="nx">bar</code> <code class="o">*</code> <code class="mi">8</code> <code class="o">*</code> <code class="nx">eighthNoteTime</code><code class="p">;</code>
  <code class="c1">// Play the bass (kick) drum on beats 1, 5</code>
  <code class="nx">playSound</code><code class="p">(</code><code class="nx">kick</code><code class="p">,</code> <code class="nx">time</code><code class="p">);</code>
  <code class="nx">playSound</code><code class="p">(</code><code class="nx">kick</code><code class="p">,</code> <code class="nx">time</code> <code class="o">+</code> <code class="mi">4</code> <code class="o">*</code> <code class="nx">eighthNoteTime</code><code class="p">);</code>

  <code class="c1">// Play the snare drum on beats 3, 7</code>
  <code class="nx">playSound</code><code class="p">(</code><code class="nx">snare</code><code class="p">,</code> <code class="nx">time</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="nx">eighthNoteTime</code><code class="p">);</code>
  <code class="nx">playSound</code><code class="p">(</code><code class="nx">snare</code><code class="p">,</code> <code class="nx">time</code> <code class="o">+</code> <code class="mi">6</code> <code class="o">*</code> <code class="nx">eighthNoteTime</code><code class="p">);</code>

  <code class="c1">// Play the hihat every eighth note.</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="mi">8</code><code class="p">;</code> <code class="o">++</code><code class="nx">i</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">playSound</code><code class="p">(</code><code class="nx">hihat</code><code class="p">,</code> <code class="nx">time</code> <code class="o">+</code> <code class="nx">i</code> <code class="o">*</code> <code class="nx">eighthNoteTime</code><code class="p">);</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
<p id="once_youve_sch">Once you’ve scheduled sound in the future, there is no way to
    unschedule that future playback event, so if you are dealing with an
    application that quickly changes, scheduling sounds too far into the
    future is not advisable. A good way of dealing with this problem is to
    create your own scheduler using JavaScript timers and an event queue. This
    approach is described in <a class="ulink" href="http://www.html5rocks.com/en/" target="_top">A Tale
    of Two Clocks</a>.</p>
<p id="para_id1" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 266px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/rhythm/index.html"> </iframe></div>
<p class="online_only">
    </p>
</div>
<div class="sect1" data-original-filename="ch02.xml" id="s02_4">
<div class="titlepage"><div><div><h2 class="title">Changing Audio Parameters</h2></div></div></div>
<p id="many_types_of_a">Many types of audio nodes have configurable parameters. For example,
    the <code class="literal">GainNode</code> has a gain parameter that
    controls the gain multiplier for all sounds going through the node.
    Specifically, a gain of 1 does not affect the amplitude, 0.5 halves it,
    and 2 doubles it [see <a class="xref" href="ch03.html#s03_1" title="Volume, Gain, and Loudness">“Volume, Gain, and Loudness”</a>]. Let’s set up a graph as
    follows:</p>
<pre class="programlisting" data-language="javascript" id="create_a_gai"><code class="c1">// Create a gain node.</code>
<code class="kd">var</code> <code class="nx">gainNode</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createGain</code><code class="p">();</code>
<code class="c1">// Connect the source to the gain node.</code>
<code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">gainNode</code><code class="p">);</code>
<code class="c1">// Connect the gain node to the destination.</code>
<code class="nx">gainNode</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code></pre>
<p id="in_the_context_">In the context of the API, audio parameters are represented as
    <code class="code">AudioParam</code> instances. The values of these nodes can be
    changed directly by setting the <code class="code">value</code> attribute of a param
    instance:</p>
<pre class="programlisting" data-language="javascript" id="reduce_the_v"><code class="c1">// Reduce the volume.</code>
<code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">;</code></pre>
<p id="the_values_can_">The values can also be changed later, via precisely scheduled
    parameter changes in the future. We could also use <code class="literal">setTimeout</code> to do this scheduling, but this is
    not precise for several reasons:</p>
<div class="orderedlist" id="millisecondbas_id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="millisecondbas_id2">Millisecond-based timing may not be enough precision.</p></li>
<li class="listitem"><p id="the_main_js_thr">The main JS thread may be busy with high-priority tasks like
        page layout, garbage collection, and callbacks from other APIs, which
        delays timers.</p></li>
<li class="listitem"><p id="the_js_timer_is">The JS timer is affected by tab state. For example, interval
        timers in backgrounded tabs fire more slowly than if the tab is in the
        foreground.</p></li>
</ol></div>
<p id="instead_of_sett">Instead of setting the value directly, we can call the
    <code class="code">setValueAtTime()</code> function, which takes a value and a start
    time as arguments. For example, the following snippet sets the gain value
    of a <code class="literal">GainNode</code> in one second:</p>
<pre class="programlisting" data-language="javascript" id="gainnodegains"><code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">setValueAtTime</code><code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code> <code class="o">+</code> <code class="mi">1</code><code class="p">);</code></pre>
<p id="para_id2" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 315px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/volume/index.html"> </iframe></div>
<p class="online_only">
    
    </p>
</div>
<div class="sect1" data-original-filename="ch02.xml" id="s02_5">
<div class="titlepage"><div><div><h2 class="title">Gradually Varying Audio Parameters</h2></div></div></div>
<p id="in_many_cases_">In many cases, rather than changing a parameter abruptly, you would
    prefer a more gradual change. For example, when building a music player
    application, we want to fade the current track out, and fade the new one
    in, to avoid a jarring transition. While you can achieve this with
    multiple calls to <code class="literal">setValueAtTime</code> as
    described previously, this is inconvenient.</p>
<p id="the_web_audio_a_id4">The Web Audio API provides a convenient set of
    <code class="code">RampToValue</code> methods to gradually change the value of any
    parameter. These functions are <code class="code">linearRampToValueAtTime()</code> and
    <code class="code">exponentialRampToValueAtTime()</code>. The difference between these
    two lies in the way the transition happens. In some cases, an exponential
    transition makes more sense, since we perceive many aspects of sound in an
    exponential manner.</p>
<p id="lets_take_an_e">Let’s take an example of scheduling a crossfade in the future. Given
    a playlist, we can transition between tracks by scheduling a gain decrease
    on the currently playing track, and a gain increase on the next one, both
    slightly before the current track finishes <span class="keep-together">playing</span>:</p>
<pre class="programlisting" data-language="javascript" id="function_create"><code class="kd">function</code> <code class="nx">createSource</code><code class="p">(</code><code class="nx">buffer</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="kd">var</code> <code class="nx">gainNode</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createGainNode</code><code class="p">();</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="nx">buffer</code><code class="p">;</code>
  <code class="c1">// Connect source to gain.</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">gainNode</code><code class="p">);</code>
  <code class="c1">// Connect gain to destination.</code>
  <code class="nx">gainNode</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>

  <code class="k">return</code> <code class="p">{</code>
    <code class="nx">source</code><code class="o">:</code> <code class="nx">source</code><code class="p">,</code>
    <code class="nx">gainNode</code><code class="o">:</code> <code class="nx">gainNode</code>
  <code class="p">};</code>
<code class="p">}</code>

<code class="kd">function</code> <code class="nx">playHelper</code><code class="p">(</code><code class="nx">buffers</code><code class="p">,</code> <code class="nx">iterations</code><code class="p">,</code> <code class="nx">fadeTime</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">currTime</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code><code class="p">;</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">iterations</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="c1">// For each buffer, schedule its playback in the future.</code>
    <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">j</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">j</code> <code class="o">&lt;</code> <code class="nx">buffers</code><code class="p">.</code><code class="nx">length</code><code class="p">;</code> <code class="nx">j</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
      <code class="kd">var</code> <code class="nx">buffer</code> <code class="o">=</code> <code class="nx">buffers</code><code class="p">[</code><code class="nx">j</code><code class="p">];</code>
      <code class="kd">var</code> <code class="nx">duration</code> <code class="o">=</code> <code class="nx">buffer</code><code class="p">.</code><code class="nx">duration</code><code class="p">;</code>
      <code class="kd">var</code> <code class="nx">info</code> <code class="o">=</code> <code class="nx">createSource</code><code class="p">(</code><code class="nx">buffer</code><code class="p">);</code>
      <code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="nx">info</code><code class="p">.</code><code class="nx">source</code><code class="p">;</code>
      <code class="kd">var</code> <code class="nx">gainNode</code> <code class="o">=</code> <code class="nx">info</code><code class="p">.</code><code class="nx">gainNode</code><code class="p">;</code>
      <code class="c1">// Fade it in.</code>
      <code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nx">currTime</code><code class="p">);</code>
      <code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nx">currTime</code> <code class="o">+</code> <code class="nx">fadeTime</code><code class="p">);</code>
      <code class="c1">// Then fade it out.</code>
      <code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nx">currTime</code> <code class="o">+</code> <code class="nx">duration</code><code class="o">-</code><code class="nx">fadeTime</code><code class="p">);</code>
      <code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nx">currTime</code> <code class="o">+</code> <code class="nx">duration</code><code class="p">);</code>

      <code class="c1">// Play the track now.</code>
      <code class="nx">source</code><code class="p">.</code><code class="nx">noteOn</code><code class="p">(</code><code class="nx">currTime</code><code class="p">);</code>

      <code class="c1">// Increment time for the next iteration.</code>
      <code class="nx">currTime</code> <code class="o">+=</code> <code class="nx">duration</code> <code class="o">-</code> <code class="nx">fadeTime</code><code class="p">;</code>
    <code class="p">}</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
<p id="para_id3" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 272px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/crossfade-playlist/index.html"> </iframe></div>
<p class="online_only">
    </p>
</div>
<div class="sect1" data-original-filename="ch02.xml" id="s02_6">
<div class="titlepage"><div><div><h2 class="title">Custom Timing Curves</h2></div></div></div>
<p id="if_neither_a_li">If neither a linear nor an exponential curve satisfies your needs,
    you can also specify your own value curve via an array of values using the
    <code class="code">setValueCurveAtTime</code> function. With this function, you can
    define a custom timing curve by providing an array of timing values. It’s
    a shortcut for making a bunch of <code class="code">setValueAtTime</code> calls, and
    should be used in this case. For example, if we want to create a tremolo
    effect, we can apply an oscillating curve to the gain
    <code class="code">AudioParam</code> of a <code class="code">GainNode</code>, as in <a class="xref" href="ch02.html#fig10" title="Figure 2-2. A value curve oscillating over time">Figure 2-2</a>.</p>
<div class="figure" id="fig10">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0202.png" alt="A value curve oscillating over time"></div></div>
<div class="figure-title">Figure 2-2. A value curve oscillating over time</div>
</div>
<p id="the_oscillating">The oscillating curve in the previous figure could be implemented
    with the following code:</p>
<pre class="programlisting" data-language="javascript" id="var_duration__"><code class="kd">var</code> <code class="nx">DURATION</code> <code class="o">=</code> <code class="mi">2</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">FREQUENCY</code> <code class="o">=</code> <code class="mi">1</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">SCALE</code> <code class="o">=</code> <code class="mf">0.4</code><code class="p">;</code>

<code class="c1">// Split the time into valueCount discrete steps.</code>
<code class="kd">var</code> <code class="nx">valueCount</code> <code class="o">=</code> <code class="mi">4096</code><code class="p">;</code>
<code class="c1">// Create a sinusoidal value curve.</code>
<code class="kd">var</code> <code class="nx">values</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Float32Array</code><code class="p">(</code><code class="nx">valueCount</code><code class="p">);</code>
<code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">valueCount</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">percent</code> <code class="o">=</code> <code class="p">(</code><code class="nx">i</code> <code class="o">/</code> <code class="nx">valueCount</code><code class="p">)</code> <code class="o">*</code> <code class="nx">DURATION</code><code class="o">*</code><code class="nx">FREQUENCY</code><code class="p">;</code>
  <code class="nx">values</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">+</code> <code class="p">(</code><code class="nb">Math</code><code class="p">.</code><code class="nx">sin</code><code class="p">(</code><code class="nx">percent</code> <code class="o">*</code> <code class="mi">2</code><code class="o">*</code><code class="nb">Math</code><code class="p">.</code><code class="nx">PI</code><code class="p">)</code> <code class="o">*</code> <code class="nx">SCALE</code><code class="p">);</code>
  <code class="c1">// Set the last value to one, to restore playbackRate to normal at the end.</code>
  <code class="k">if</code> <code class="p">(</code><code class="nx">i</code> <code class="o">==</code> <code class="nx">valueCount</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">values</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code><code class="p">;</code>
  <code class="p">}</code>
<code class="p">}</code>
<code class="c1">// Apply it to the gain node immediately, and make it last for 2 seconds.</code>
<code class="k">this</code><code class="p">.</code><code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">setValueCurveAtTime</code><code class="p">(</code><code class="nx">values</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code><code class="p">,</code> <code class="nx">DURATION</code><code class="p">);</code></pre>
<p id="in_the_previous_id1">In the previous snippet, we’ve manually computed a sine curve and
    applied it to the gain parameter to create a tremolo sound effect. It took
    a bit of math, though.</p>
<p id="this_brings_us_">This brings us to a very nifty feature of the Web Audio API that
    lets us build effects like tremolo more easily. We can take any audio
    stream that would ordinarily be connected into another
    <code class="code">AudioNode</code>, and instead connect it into any
    <code class="code">AudioParam</code>. This important idea is the basis for many sound
    effects. The previous code is actually an example of such an effect called
    a low frequency oscillator (LFO) applied to the gain, which is used to
    build effects such as vibrato, phasing, and tremolo. By using the
    oscillator node [see <a class="xref" href="ch04.html#s04_5" title="Oscillator-Based Direct Sound Synthesis">“Oscillator-Based Direct Sound Synthesis”</a>], we can easily rebuild the
    previous example as follows:</p>
<pre class="programlisting" data-language="javascript" id="create_oscil"><code class="c1">// Create oscillator.</code>
<code class="kd">var</code> <code class="nx">osc</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createOscillator</code><code class="p">();</code>
<code class="nx">osc</code><code class="p">.</code><code class="nx">frequency</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nx">FREQUENCY</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">gain</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createGain</code><code class="p">();</code>
<code class="nx">gain</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nx">SCALE</code><code class="p">;</code>
<code class="nx">osc</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">gain</code><code class="p">);</code>
<code class="nx">gain</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="k">this</code><code class="p">.</code><code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">);</code>

<code class="c1">// Start immediately, and stop in 2 seconds.</code>
<code class="nx">osc</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="nx">osc</code><code class="p">.</code><code class="nx">stop</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code> <code class="o">+</code> <code class="nx">DURATION</code><code class="p">);</code></pre>
<p id="the_latter_appr">The latter approach is more efficient than creating a custom value
    curve and saves us from having to compute sine functions manually by
    creating a loop to repeat the effect.</p>
</div></section><section class="chapter" data-original-filename="ch03.xml" id="ch03"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Volume and Loudness</h1></div></div></div>
<p id="once_we_are_rea">Once we are ready to play a sound, whether from an
  <code class="code">AudioBuffer</code> or from other sources, one of the most basic
  parameters we can change is the loudness of the sound.</p>
<p id="the_main_way_to">The main way to affect the loudness of a sound is using
  <code class="code">GainNodes</code>. As previously mentioned, these nodes have a gain
  parameter, which acts as a multiplier on the incoming sound buffer. The
  default gain value is one, which means that the input sound is unaffected.
  Values between zero and one reduce the loudness, and values greater than one
  amplify the loudness. Negative gain (values less than zero) inverts the
  waveform (i.e., the amplitude is flipped).</p>
<div class="sidebar theory" id="s03_1">
<div class="titlepage"><div><div><div class="sidebar-title">Volume, Gain, and Loudness</div></div></div></div>
<p id="lets_start_wit">Let’s start with some definitions. <span class="emphasis"><em>Loudness</em></span> is
    a subjective measure of how intensely our ears perceive a sound.
    <span class="emphasis"><em>Volume</em></span> is a measure of the physical amplitude of a
    sound wave. <span class="emphasis"><em>Gain</em></span> is a scale multiplier affecting a
    sound’s amplitude as it is being processed.</p>
<p id="in_other_words">In other words, when undergoing a gain, the amplitude of a sound
    wave is scaled, with the gain value used as a multiplier. For example,
    while a gain value of one will not affect the sound wave at all, <a class="xref" href="ch03.html#fig11" title="Figure 3-1. Original soundform on the left, gain 2 soundform on the right">Figure 3-1</a> illustrates what happens to a sound wave if you send it
    through a gain factor of two.</p>
<div class="figure" id="fig11">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0301.png" alt="Original soundform on the left, gain 2 soundform on the right"></div></div>
<div class="figure-title">Figure 3-1. Original soundform on the left, gain 2 soundform on the
      right</div>
</div>
<p id="generally_speak_id1">Generally speaking, power in a wave is measured in decibels
    (abbreviated dB), or one tenth of a Bel, named after Alexander Graham
    Bell. Decibels are a relative, logarithmic unit that compare the level
    being measured to some reference point. There are many different reference
    points for measuring dB, and each reference point is indicated with a
    suffix on the unit. Saying that a signal is some number of dB is
    meaningless without a reference point! For example, dBV, dBu, and dBm are
    all useful for measuring electrical signals. Since we are dealing with
    digital audio, we are mainly concerned with two measures: dBFS and
    dBSPL.</p>
<p id="the_first_is_db">The first is <span class="emphasis"><em>dBFS</em></span>, or decibels full scale. The
    highest possible level of sound produced by audio equipment is 0 dBFS. All
    other levels are expressed in negative numbers.</p>
<p id="dbfs_is_describ">dBFS is described mathematically as:</p>
<pre class="programlisting" data-language="javascript" id="dbfs____log"><code class="nx">dBFS</code> <code class="o">=</code> <code class="mi">20</code> <code class="o">*</code> <code class="nx">log</code><code class="p">(</code> <code class="p">[</code><code class="nx">sample</code> <code class="nx">level</code><code class="p">]</code> <code class="o">/</code> <code class="p">[</code><code class="nx">max</code> <code class="nx">level</code><code class="p">]</code> <code class="p">)</code></pre>
<p id="the_maximum_dbf">The maximum dBFS value in a 16-bit audio system is:</p>
<pre class="programlisting" data-language="javascript" id="max____log"><code class="nx">max</code> <code class="o">=</code> <code class="mi">20</code> <code class="o">*</code> <code class="nx">log</code><code class="p">(</code><code class="mi">1111</code> <code class="mi">1111</code> <code class="mi">1111</code> <code class="mi">1111</code><code class="o">/</code><code class="mi">1111</code> <code class="mi">1111</code> <code class="mi">1111</code> <code class="mi">1111</code><code class="p">)</code> <code class="o">=</code> <code class="nx">log</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code> <code class="o">=</code> <code class="mi">0</code></pre>
<p id="note_that_the_m">Note that the maximum dBFS value will always be 0 by definition,
    since log(1) = 0. Similarly, the minimum dBFS value in the same system
    is:</p>
<pre class="programlisting" data-language="javascript" id="min____log"><code class="nx">min</code> <code class="o">=</code> <code class="mi">20</code> <code class="o">*</code> <code class="nx">log</code><code class="p">(</code><code class="mi">0000</code> <code class="mi">0000</code> <code class="mi">0000</code> <code class="mi">0001</code><code class="o">/</code><code class="mi">1111</code> <code class="mi">1111</code> <code class="mi">1111</code> <code class="mi">1111</code><code class="p">)</code> <code class="o">=</code> <code class="o">-</code><code class="mi">96</code> <code class="nx">dBFS</code></pre>
<p id="dbfs_is_a_measu">dBFS is a measure of gain, not volume. You can play a 0-dBFS signal
    through your stereo with the stereo gain set very low and hardly be able
    to hear anything. Conversely, you can play a −30-dBFS signal with the
    stereo gain maxed and blow your eardrums away.</p>
<p id="that_said_you">That said, you’ve probably heard someone describe the volume of a
    sound in decibels. Technically speaking, they were referring to
    <span class="emphasis"><em>dBSPL</em></span>, or decibels relative to sound pressure level.
    Here, the reference point is 0.000002 newtons per square meter (roughly
    the sound of a mosquito flying 3 m away). There is no upper value to
    dBSPL, but in practice, we want to stay below levels of ear damage (~120
    dBSPL) and well below the threshold of pain (~150 dBSPL). The Web Audio
    API does not use dBSPL, since the final volume of the sound depends on the
    OS gain and the speaker gain, and only deals with dBFS.</p>
<p id="the_logarithmic">The logarithmic definition of decibels correlates somewhat to the
    way our ears perceive loudness, but loudness is still a very subjective
    concept. Comparing the dB values of a sound and the same sound with a 2x
    gain, we can see that we’ve gained about 6 dB:</p>
<pre class="programlisting" data-language="javascript" id="diff____log"><code class="nx">diff</code> <code class="o">=</code> <code class="mi">20</code> <code class="o">*</code> <code class="nx">log</code><code class="p">(</code><code class="mi">2</code><code class="o">/</code><code class="mi">2</code><code class="o">^</code><code class="mi">16</code><code class="p">)</code> <code class="o">-</code> <code class="mi">20</code> <code class="o">*</code> <code class="nx">log</code><code class="p">(</code><code class="mi">1</code><code class="o">/</code><code class="mi">2</code><code class="o">^</code><code class="mi">16</code><code class="p">)</code> <code class="o">=</code> <code class="mf">6.02</code> <code class="nx">dB</code></pre>
<p id="every_time_we_a">Every time we add 6 dB or so, we actually double the amplitude of
    the signal. Comparing the sound at a rock concert (~110 dBSPL) to your
    alarm clock (~80 dBSPL), the difference between the two is (110 − 80)/6
    dB, or roughly 5 times louder, with a gain multiplier of
    2<sup>5</sup> = 32x. A volume knob on a stereo is
    therefore also calibrated to increase the amplitude exponentially. In
    other words, turning the volume knob by 3 units multiplies the amplitude
    of the signal roughly by a factor of 2<sup>3</sup> or 8
    times. In practice, the exponential model described here is merely an
    approximation to the way our ears perceive loudness, and audio equipment
    manufacturers often have their own custom gain curves that are neither
    linear nor exponential.</p>
</div>
<div class="sect1" data-original-filename="ch03.xml" id="s03_2">
<div class="titlepage"><div><div><h2 class="title">Equal Power Crossfading</h2></div></div></div>
<p id="often_in_a_game">Often in a game setting, you have a situation where you want to
    crossfade between two environments that have different sounds associated
    with them. However, when to crossfade and by how much is not known in
    advance; perhaps it varies with the position of the game avatar, which is
    controlled by the player. In this case, we cannot do an automatic
    ramp.</p>
<p id="in_general_doi">In general, doing a straightforward, linear fade will result in the
    following graph. It can sound unbalanced because of a volume dip between
    the two samples, as shown in <a class="xref" href="ch03.html#fig12" title="Figure 3-2. A linear crossfade between two tracks">Figure 3-2</a>.</p>
<div class="figure" id="fig12">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0302.png" alt="A linear crossfade between two tracks"></div></div>
<div class="figure-title">Figure 3-2. A linear crossfade between two tracks</div>
</div>
<p id="to_address_this">To address this issue, we use an equal power curve, in which the
    corresponding gain curves are neither linear nor exponential, and
    intersect at a higher amplitude (<a class="xref" href="ch03.html#fig13" title="Figure 3-3. An equal power crossfade sounds much better">Figure 3-3</a>). This helps
    avoid a dip in volume in the middle part of the crossfade, when both
    sounds are mixed together equally.</p>
<div class="figure" id="fig13">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0303.png" alt="An equal power crossfade sounds much better"></div></div>
<div class="figure-title">Figure 3-3. An equal power crossfade sounds much better</div>
</div>
<p id="the_graph_in_ca">The graph in <a class="xref" href="ch03.html#fig13" title="Figure 3-3. An equal power crossfade sounds much better">Figure 3-3</a> can be generated with a bit of
    math:</p>
<pre class="programlisting" data-language="javascript" id="function_equalp"><code class="kd">function</code> <code class="nx">equalPowerCrossfade</code><code class="p">(</code><code class="nx">percent</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Use an equal-power crossfading curve:</code>
  <code class="kd">var</code> <code class="nx">gain1</code> <code class="o">=</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">cos</code><code class="p">(</code><code class="nx">percent</code> <code class="o">*</code> <code class="mf">0.5</code><code class="o">*</code><code class="nb">Math</code><code class="p">.</code><code class="nx">PI</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">gain2</code> <code class="o">=</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">cos</code><code class="p">((</code><code class="mf">1.0</code> <code class="o">-</code> <code class="nx">percent</code><code class="p">)</code> <code class="o">*</code> <code class="mf">0.5</code><code class="o">*</code><code class="nb">Math</code><code class="p">.</code><code class="nx">PI</code><code class="p">);</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">ctl1</code><code class="p">.</code><code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nx">gain1</code><code class="p">;</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">ctl2</code><code class="p">.</code><code class="nx">gainNode</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nx">gain2</code><code class="p">;</code>
<code class="p">}</code></pre>
<p id="para_id4" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 293px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/crossfade/index.html"> </iframe></div>
<p class="online_only">
    </p>
<div class="sidebar theory" id="s03_3">
<div class="titlepage"><div><div><div class="sidebar-title">Clipping and Metering</div></div></div></div>
<p id="like_images_exc">Like images exceeding the boundaries of a canvas, sounds can also
      be clipped if the waveform exceeds its maximum level. The distinct
      distortion that this produces is obviously undesirable. Audio equipment
      often has indicators that show the magnitude of audio levels to help
      engineers and listeners produce output that does not clip. These
      indicators are called meters (<a class="xref" href="ch03.html#fig14" title="Figure 3-4. A meter in a typical receiver">Figure 3-4</a>) and often have a
      green zone (no clipping), yellow zone (close to clipping), and red zone
      (clipping).</p>
<div class="figure" id="fig14">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0304.png" alt="A meter in a typical receiver"></div></div>
<div class="figure-title">Figure 3-4. A meter in a typical receiver</div>
</div>
<p id="clipped_sound_l">Clipped sound looks bad on a monitor and sounds no better. It’s
      important to listen for harsh distortions, or conversely, overly subdued
      mixes that force your listeners to crank up the volume. If you’re in
      either of these situations, read on!</p>
</div>
</div>
<div class="sect1" data-original-filename="ch03.xml" id="s03_4">
<div class="titlepage"><div><div><h2 class="title">Using Meters to Detect and Prevent Clipping</h2></div></div></div>
<p id="since_multiple_">Since multiple sounds playing simultaneously are additive with no
    level reduction, you may find yourself in a situation where you are
    exceeding past the threshold of your speaker’s capability. The maximum
    level of sound is 0 dBFS, or 2<sup>16</sup>, for 16-bit
    audio. In the floating point version of the signal, these bit values are
    mapped to [−1, 1]. The waveform of a sound that’s being clipped looks
    something like <a class="xref" href="ch03.html#fig15" title="Figure 3-5. A diagram of a waveform being clipped">Figure 3-5</a>. In the context of the Web Audio
    API, sounds clip if the values sent to the destination node lie outside of
    the range. It’s a good idea to leave some room (called
    <span class="emphasis"><em>headroom</em></span>) in your final mix so that you aren’t too
    close to the clipping threshold.</p>
<div class="figure" id="fig15">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0305.png" alt="A diagram of a waveform being clipped"></div></div>
<div class="figure-title">Figure 3-5. A diagram of a waveform being clipped</div>
</div>
<p id="in_addition_to__id1">In addition to close listening, you can check whether or not you are
    clipping your sound programmatically by putting a script processor node
    into your audio graph. Clipping may occur if any of the PCM values are out
    of the acceptable range. In this sample, we check both left and right
    channels for clipping, and if clipping is detected, save the last clipping
    time:</p>
<pre class="programlisting" data-language="javascript" id="function_onproc_id1"><code class="kd">function</code> <code class="nx">onProcess</code><code class="p">(</code><code class="nx">e</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">leftBuffer</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">inputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">rightBuffer</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">inputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">1</code><code class="p">);</code>
  <code class="nx">checkClipping</code><code class="p">(</code><code class="nx">leftBuffer</code><code class="p">);</code>
  <code class="nx">checkClipping</code><code class="p">(</code><code class="nx">rightBuffer</code><code class="p">);</code>
<code class="p">}</code>

<code class="kd">function</code> <code class="nx">checkClipping</code><code class="p">(</code><code class="nx">buffer</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">isClipping</code> <code class="o">=</code> <code class="kc">false</code><code class="p">;</code>
  <code class="c1">// Iterate through buffer to check if any of the |values| exceeds 1.</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">buffer</code><code class="p">.</code><code class="nx">length</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="kd">var</code> <code class="nx">absValue</code> <code class="o">=</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">abs</code><code class="p">(</code><code class="nx">buffer</code><code class="p">[</code><code class="nx">i</code><code class="p">]);</code>
    <code class="k">if</code> <code class="p">(</code><code class="nx">absValue</code> <code class="o">&gt;=</code> <code class="mf">1.0</code><code class="p">)</code> <code class="p">{</code>
      <code class="nx">isClipping</code> <code class="o">=</code> <code class="kc">true</code><code class="p">;</code>
      <code class="k">break</code><code class="p">;</code>
    <code class="p">}</code>
  <code class="p">}</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">isClipping</code> <code class="o">=</code> <code class="nx">isClipping</code><code class="p">;</code>
  <code class="k">if</code> <code class="p">(</code><code class="nx">isClipping</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">lastClipTime</code> <code class="o">=</code> <code class="k">new</code> <code class="nb">Date</code><code class="p">();</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
<p id="an_alternative_">An alternative implementation of metering could poll a real-time
    analyzer in the audio graph for <code class="code">getFloatFrequencyData</code> at
    render time, as determined by <code class="code">requestAnimationFrame</code> (see
    <a class="xref" href="ch05.html" title="Chapter 5. Analysis and Visualization">Chapter 5</a>). This approach is more efficient, but misses a
    lot of the signal (including places where it potentially clips), since
    rendering happens most at 60 times a second, whereas the audio signal
    changes far more quickly.</p>
<p id="the_way_to_prev">The way to prevent clipping is to reduce the overall level of the
    signal. If you are clipping, apply some fractional gain on a master audio
    gain node to subdue your mix to a level that prevents clipping. In
    general, you should tweak gains to anticipate the worst case, but getting
    this right is more of an art than a science. In practice, since the sounds
    playing in your game or interactive application may depend on a huge
    variety of factors that are decided at runtime, it can be difficult to
    pick the master gain value that prevents clipping in all cases. For this
    unpredictable case, look to dynamics compression, which is discussed in
    <a class="xref" href="ch03.html#s03_6" title="Dynamics Compression">“Dynamics Compression”</a>.</p>
<p id="para_id5" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 297px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/metering/index.html"> </iframe></div>
<p class="online_only">
    </p>
<div class="sidebar theory" id="s03_5">
<div class="titlepage"><div><div><div class="sidebar-title">Understanding Dynamic Range</div></div></div></div>
<p id="in_audio_dynam">In audio, <span class="emphasis"><em>dynamic range</em></span> refers to the
      difference between the loudest and quietest parts of a sound. The amount
      of dynamic range in musical pieces varies greatly depending on genre.
      Classical music has large dynamic range and often features very quiet
      sections followed by relatively loud ones. Many popular genres like rock
      and electronica tend to have a small dynamic range, and are uniformly
      loud because of an apparent competition (known pejoratively as the
      “Loudness War”) to increase the loudness of tracks to meet consumer
      demands. This uniform loudness is generally achieved by using dynamic
      range compression.</p>
<p id="that_said_ther">That said, there are many legitimate uses of compression.
      Sometimes recorded music has such a large dynamic range that there are
      sections that sound so quiet or loud that the listener constantly needs
      to have a finger on the volume knob. Compression can quiet down the loud
      parts while making the quiet parts audible. <a class="xref" href="ch03.html#fig16" title="Figure 3-6. The effects of dynamics compression">Figure 3-6</a>
      illustrates a waveform (above), and then the same waveform with
      compression applied (below). You can see that the sound is louder
      overall, and there is less variance in the amplitude.</p>
<div class="figure" id="fig16">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0306.png" alt="The effects of dynamics compression"></div></div>
<div class="figure-title">Figure 3-6. The effects of dynamics compression</div>
</div>
<p id="for_games_and_i">For games and interactive applications, you may not know
      beforehand what your sound output will look like. Because of games’
      dynamic nature, you may have very quiet periods (e.g., stealthy
      sneaking) followed by very loud ones (e.g., a warzone). A compressor
      node can be helpful in suddenly loud situations for reducing the
      likelihood of clipping [see <a class="xref" href="ch03.html#s03_3" title="Clipping and Metering">“Clipping and Metering”</a>].</p>
<p id="compressors_can">Compressors can be modeled with a compression curve with several
      parameters, all of which can be tweaked with the Web Audio API. Two of
      the main parameters of a <span class="keep-together">compressor</span> are threshold and ratio.
      Threshold refers to the lowest volume at which a compressor starts
      reducing dynamic range. Ratio determines how much gain reduction is
      applied by the compressor. <a class="xref" href="ch03.html#fig17" title="Figure 3-7. A sample compression curve with basic parameters">Figure 3-7</a> illustrates the
      effect of threshold and various compression ratios on the compression
      curve.</p>
<div class="figure" id="fig17">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0307.png" alt="A sample compression curve with basic parameters"></div></div>
<div class="figure-title">Figure 3-7. A sample compression curve with basic parameters</div>
</div>
</div>
</div>
<div class="sect1" data-original-filename="ch03.xml" id="s03_6">
<div class="titlepage"><div><div><h2 class="title">Dynamics Compression</h2></div></div></div>
<p id="compressors_are">Compressors are available in the Web Audio API as
    <code class="code">DynamicsCompressorNodes</code>. Using moderate amounts of dynamics
    compression in your mix is generally a good idea, especially in a game
    setting where, as previously discussed, you don’t know exactly what sounds
    will play and when. One case where compression should be avoided is when
    dealing with painstakingly mastered tracks that have been tuned to sound
    “just right” already, which are not being mixed with any other
    tracks.</p>
<p id="implementing_dy">Implementing dynamic compression in the Web Audio API is simply a
    matter of including a dynamics compressor node in your audio graph,
    generally as the last node before the destination:</p>
<pre class="programlisting" data-language="javascript" id="var_compressor_"><code class="kd">var</code> <code class="nx">compressor</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createDynamicsCompressor</code><code class="p">();</code>
<code class="nx">mix</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">compressor</code><code class="p">);</code>
<code class="nx">compressor</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code></pre>
<p id="the_node_can_be">The node can be configured with some additional parameters as
    described in the theory section, but the defaults are quite good for most
    purposes. For more information about configuring the compression curve,
    see the <a class="ulink" href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html" target="_top">Web
    Audio API specification</a>.</p>
</div></section><section class="chapter" data-original-filename="ch04.xml" id="ch04"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Pitch and the Frequency Domain</h1></div></div></div>
<p id="so_far_we_have_">So far we have learned about some basic properties of sound: timing
  and volume. To do more complex things, such as sound equalization (e.g.,
  increasing the bass and decreasing the treble), we need more complex tools.
  This section explains some of the tools that allow you to do these more
  interesting transformations, which include the ability to simulate different
  sorts of environments and manipulate sounds directly with JavaScript.</p>
<div class="sidebar theory" id="s04_1">
<div class="titlepage"><div><div><div class="sidebar-title">Basics of Musical Pitch</div></div></div></div>
<p id="music_consists_">Music consists of many sounds played simultaneously. Sounds produced
    by musical instruments can be very complex, as the sound bounces through
    various parts of the instrument and is shaped in unique ways. However,
    these musical tones all have one thing in common: physically, they are
    periodic waveforms. This periodicity is perceived by our ears as pitch.
    The pitch of a note is measured in frequency, or the number of times the
    wave pattern repeats every second, specified in
    <span class="emphasis"><em>hertz</em></span>. The frequency is the time (in seconds) between
    the crests of the wave. As illustrated in <a class="xref" href="ch04.html#fig18" title="Figure 4-1. Graph of perfect A4 and A5 tones side by side">Figure 4-1</a>, if we
    halve the wave in the time dimension, we end up with a correspondingly
    doubled frequency, which sounds to our ears like the same tone, one octave
    higher. Conversely, if we extend the wave’s frequency by two, this brings
    the tone an octave down. Thus, pitch (like volume) is perceived
    exponentially by our ears: at every octave, the frequency doubles.</p>
<div class="figure" id="fig18">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0401.png" alt="Graph of perfect A4 and A5 tones side by side"></div></div>
<div class="figure-title">Figure 4-1. Graph of perfect A4 and A5 tones side by side</div>
</div>
<p id="octaves_are_spl">Octaves are split up into 12 semitones. Each adjacent semitone pair
    has an identical frequency ratio (at least in equal-tempered tunings). In
    other words, the ratios of the frequencies of A4 to A#4 are identical to
    A#4 to B.</p>
<p id="shows_how_we_wo"><a class="xref" href="ch04.html#fig18" title="Figure 4-1. Graph of perfect A4 and A5 tones side by side">Figure 4-1</a> shows how we would derive the ratio between
    every successive semitone, given that:</p>
<div class="orderedlist" id="to_transpose_a__id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="to_transpose_a__id2">To transpose a note up an octave, we double the frequency of the
        note.</p></li>
<li class="listitem"><p id="each_octave_is_">Each octave is split up into 12 semitones, which, in an equal
        tempered tuning, have identical frequency ratios.</p></li>
</ol></div>
<p id="lets_define_a_">Let's define a <span class="mathphrase">$f_0$</span> to be some frequency, and <span class="mathphrase">$f_1$</span> to be that same note one octave higher. we know that
    this is the relationship between them:</p>
<div class="informalequation" id="beginequation_id1"><span class="mathphrase">\begin{equation} {f_1 = 2 * f_0}
      \end{equation}</span></div>
<p id="next_let_k_be_">Next, let <span class="emphasis"><em>k</em></span> be the fixed multiplier between any
    two adjacent semitones. Since there are 12 semitones in an octave, we also
    know the following:</p>
<div class="informalequation" id="beginequation_id2"><span class="mathphrase">\begin{equation} {f_1 = f_0 * k*k*k*...*k (12x) =
      f_0 * k^{12}} \end{equation}</span></div>
<p id="solving_the_sys">Solving the system of equations above, we have the following:</p>
<div class="informalequation" id="beginequation_id3"><span class="mathphrase">\begin{equation} {2 * f_0 = f_0 * k^{12}}
      \end{equation}</span></div>
<p id="solving_for_k">Solving for <span class="emphasis"><em>k</em></span>:</p>
<div class="informalequation" id="beginequation_id4"><span class="mathphrase">\begin{equation} {k = 2^{(1/12)} ~= 1.0595...}
      \end{equation}</span></div>
<p id="conveniently_a">Conveniently, all of this semitone-related offsetting isn’t really
    necessary to do manually, since many audio environments (the Web Audio API
    included) include a notion of detune, which linearizes the frequency
    domain. Detune is measured in <span class="emphasis"><em>cents</em></span>, with each octave
    consisting of 1200 cents, and each semitone consisting of 100 cents. By
    specifying a detune of 1200, you move up an octave. Specifying a detune of
    −1200 moves you down an octave.</p>
</div>
<div class="sect1" data-original-filename="ch04.xml" id="s04_2">
<div class="titlepage"><div><div><h2 class="title">Pitch and playbackRate</h2></div></div></div>
<p id="the_web_audio_a_id5">The Web Audio API provides a <code class="code">playbackRate</code> parameter on
    each <code class="literal">AudioSourceNode</code>. This value can be set to affect
    the pitch of any sound buffer. Note that the pitch as well as the duration
    of the sample will be affected in this case. There are sophisticated
    methods that try to affect pitch independent of duration, but this is
    quite difficult to do in a general-purpose way without introducing blips,
    scratches, and other undesirable artifacts to the mix.</p>
<p id="as_discussed_in">As discussed in <a class="xref" href="ch04.html#s04_1" title="Basics of Musical Pitch">“Basics of Musical Pitch”</a>, to compute the frequencies
    of successive semitones, we simply multiply the frequency by the semitone
    ratio 2<sup>1/12</sup>. This is very useful if you are
    developing a musical instrument or using pitch for randomization in a game
    setting. The following code plays a tone at a given frequency offset in
    <span class="keep-together">semitones</span>:</p>
<pre class="programlisting" data-language="javascript" id="function_playno_id1"><code class="kd">function</code> <code class="nx">playNote</code><code class="p">(</code><code class="nx">semitones</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Assume a new source was created from a buffer.</code>
  <code class="kd">var</code> <code class="nx">semitoneRatio</code> <code class="o">=</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">pow</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="o">/</code><code class="mi">12</code><code class="p">);</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">playbackRate</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">pow</code><code class="p">(</code><code class="nx">semitoneRatio</code><code class="p">,</code> <code class="nx">semitones</code><code class="p">);</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="as_we_discussed_id1">As we discussed earlier, our ears perceive pitch exponentially.
    Treating pitch as an exponential quantity can be inconvenient, since we
    often deal with awkward values such as the twelfth root of two. Instead of
    doing that, we can use the detune parameter to specify our offset in
    cents. Thus you can rewrite the above function using detune in an easier
    way:</p>
<pre class="programlisting" data-language="javascript" id="function_playno_id2"><code class="kd">function</code> <code class="nx">playNote</code><code class="p">(</code><code class="nx">semitones</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Assume a new source was created from a buffer.</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">detune</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nx">semitones</code> <code class="o">*</code> <code class="mi">100</code><code class="p">;</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="if_you_pitch_sh">If you pitch shift by too many semitones (e.g., by calling
    <code class="code">playNote(24);</code>), you will start to hear distortions. Because
    of this, digital pianos include multiple samples for each instrument. Good
    digital pianos avoid pitch bending at all, and include a separate sample
    recorded specifically for each key. Great digital pianos often include
    multiple samples for each key, which are played back depending on the
    velocity of the key press.</p>
</div>
<div class="sect1" data-original-filename="ch04.xml" id="s04_3">
<div class="titlepage"><div><div><h2 class="title">Multiple Sounds with Variations</h2></div></div></div>
<p id="a_key_feature_o">A key feature of sound effects in games is that there can be many of
    them simultaneously. Imagine you’re in the middle of a gunfight with
    multiple actors shooting machine guns. Each machine gun fires many times
    per second, causing tens of sound effects to be played at the same time.
    Playing back sound from multiple, precisely-timed sources simultaneously
    is one place the Web Audio API really shines.</p>
<p id="now_if_all_of_">Now, if all of the machine guns in your game sounded exactly the
    same, that would be pretty boring. Of course the sound would vary based on
    distance from the target and relative position [more on this later in
    <a class="xref" href="ch06.html#s06_5" title="Spatialized Sound">“Spatialized Sound”</a>], but even that might not be enough. Luckily the
    Web Audio API provides a way to easily tweak the previous example in at
    least two simple ways:</p>
<div class="orderedlist" id="with_a_subtle_s_id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="with_a_subtle_s_id2">With a subtle shift in time between bullets firing</p></li>
<li class="listitem"><p id="by_changing_pit">By changing pitch to better simulate the randomness of the real
        world</p></li>
</ol></div>
<p id="using_our_knowl">Using our knowledge of timing and pitch, implementing these two
    effects is pretty straightforward:</p>
<pre class="programlisting" data-language="javascript" id="function_shootr"><code class="kd">function</code> <code class="nx">shootRound</code><code class="p">(</code><code class="nx">numberOfRounds</code><code class="p">,</code> <code class="nx">timeBetweenRounds</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">time</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code><code class="p">;</code>
  <code class="c1">// Make multiple sources using the same buffer and play in quick succession.</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">numberOfRounds</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="k">this</code><code class="p">.</code><code class="nx">makeSource</code><code class="p">(</code><code class="nx">bulletBuffer</code><code class="p">);</code>
    <code class="nx">source</code><code class="p">.</code><code class="nx">playbackRate</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">+</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()</code> <code class="o">*</code> <code class="nx">RANDOM_PLAYBACK</code><code class="p">;</code>
    <code class="nx">source</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="nx">time</code> <code class="o">+</code> <code class="nx">i</code> <code class="o">*</code> <code class="nx">timeBetweenRounds</code> <code class="o">+</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()</code> <code class="o">*</code> <code class="nx">RANDOM_VOLUME</code><code class="p">);</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
<p id="the_web_audio_a_id6">The Web Audio API automatically merges multiple sounds playing at
    once, essentially just adding the waveforms together. This can cause
    problems such as clipping, which we discuss in <a class="xref" href="ch03.html#s03_3" title="Clipping and Metering">“Clipping and Metering”</a>.</p>
<p id="this_example_ad">This example adds some variety to <code class="code">AudioBuffers</code> loaded
    from sound files. In some cases, it is desirable to have fully synthesized
    sound effects and no buffers at all [see <a class="xref" href="ch06.html#s06_3" title="Procedurally Generated Sound">“Procedurally Generated Sound”</a>].</p>
<p id="para_id6" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 380px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/rapid-sounds/index.html"> </iframe></div>
<p class="online_only">
    </p>
<div class="sidebar theory" id="s04_4">
<div class="titlepage"><div><div><div class="sidebar-title">Understanding the Frequency Domain</div></div></div></div>
<p id="so_far_in_our_t">So far in our theoretical excursions, we’ve only examined sound as
      a function of pressure as it varies over time. Another useful way of
      looking at sound is to plot amplitude and see how it varies over
      frequency. This results in graphs where the domain (x-axis) is in units
      of frequency (Hz). Graphs of sound plotted this way are said to be in
      the <span class="emphasis"><em>frequency domain</em></span>.</p>
<p id="the_relationshi">The relationship between the time-domain and frequency-domain
      graphs is based on the idea of <span class="emphasis"><em>fourier
      decomposition</em></span>. As we saw earlier, sound waves are often
      cyclical in nature. Mathematically, periodic sound waves can be seen as
      the sum of multiple simple sine waves of different frequency and
      amplitude. The more such sine waves we add together, the better an
      approximation of the original function we can get. We can take a signal
      and find its component sine waves by applying a fourier transformation,
      the details of which are outside the scope of this book. Many algorithms
      exist to get this decomposition too, the best known of which is the Fast
      Fourier Transform (FFT). Luckily, the Web Audio API comes with an
      implementation of this algorithm. We will discuss how it works later
      [see <a class="xref" href="ch05.html#s05_1" title="Frequency Analysis">“Frequency Analysis”</a>].</p>
<p id="in_general_we_">In general, we can take a sound wave, figure out the constituent
      sine wave breakdown, and plot the (frequency, amplitude) as points on a
      new graph to get a frequency domain plot. <a class="xref" href="ch04.html#fig19" title="Figure 4-2. A perfectly sinusoidal 1-KHz sound wave represented in both time and frequency domains">Figure 4-2</a> shows
      a pure A note at 440 Hz (called A4).</p>
<div class="figure" id="fig19">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0402.png" alt="A perfectly sinusoidal 1-KHz sound wave represented in both time and frequency domains"></div></div>
<div class="figure-title">Figure 4-2. A perfectly sinusoidal 1-KHz sound wave represented in both
        time and frequency domains</div>
</div>
<p id="looking_at_the_">Looking at the frequency domain can give a better sense of the
      qualities of the sound, including pitch content, amount of noise, and
      much more. Advanced algorithms like pitch detection can be built on top
      of the frequency domain. Sound produced by real musical instruments have
      overtones, so an A4 played by a piano has a frequency domain plot that
      looks (and sounds) very different from the same A4 pitch played by a
      trumpet. Regardless of the complexity of sounds, the same fourier
      decomposition ideas apply. <a class="xref" href="ch04.html#fig20" title="Figure 4-3. A complex sound wave shown in both time and frequency domains">Figure 4-3</a> shows a more complex
      fragment of a sound in both the time and frequency domains.</p>
<div class="figure" id="fig20">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0403.png" alt="A complex sound wave shown in both time and frequency domains"></div></div>
<div class="figure-title">Figure 4-3. A complex sound wave shown in both time and frequency
        domains</div>
</div>
<p id="these_graphs_be">These graphs behave quite differently over time. If you were to
      very slowly play back the sound in <a class="xref" href="ch04.html#fig20" title="Figure 4-3. A complex sound wave shown in both time and frequency domains">Figure 4-3</a> and observe
      it moving along each graph, you would notice the time domain graph (on
      the left) progressing left to right. The frequency domain graph (on the
      right) is the frequency analysis of the waveform at a moment in time, so
      it might change more quickly and less predictably.</p>
<p id="importantly_fr">Importantly, frequency-domain analysis is still useful when the
      sound examined is not perceived as having a specific pitch. Wind,
      percussive sources, and gunshots have distinct representations in the
      frequency domain. For example, white noise has a flat frequency domain
      spectrum, since each frequency is equally represented.</p>
</div>
</div>
<div class="sect1" data-original-filename="ch04.xml" id="s04_5">
<div class="titlepage"><div><div><h2 class="title">Oscillator-Based Direct Sound Synthesis</h2></div></div></div>
<p id="as_we_discussed_id2">As we discussed early in this book, digital sound in the Web Audio
    API is represented as an array of floats in <code class="code">AudioBuffers</code>.
    Most of the time, the buffer is created by loading a sound file, or on the
    fly from some sound stream. In some cases, we might want to synthesize our
    own sounds. We can do this by creating audio buffers programmatically
    using JavaScript, which simply evaluate a mathematical function at regular
    periods and assign values to an array. By taking this approach, we can
    manually change the amplitude and frequency of our sine wave, or even
    concatenate multiple sine waves together to create arbitrary sounds
    [recall the principles of fourier transformations from <a class="xref" href="ch04.html#s04_4" title="Understanding the Frequency Domain">“Understanding the Frequency Domain”</a>].</p>
<p id="though_possible">Though possible, doing this work in JavaScript is inefficient and
    complex. Instead, the Web Audio API provides primitives that let you do
    this with oscillators: <code class="code">OscillatorNode</code>. These nodes have
    configurable frequency and detune [see the <a class="xref" href="ch04.html#s04_1" title="Basics of Musical Pitch">“Basics of Musical Pitch”</a>]. They
    also have a type that represents the kind of wave to generate. Built-in
    types include the sine, triangle, sawtooth, and square waves, as shown in
    <a class="xref" href="ch04.html#fig21" title="Figure 4-4. Types of basic soundwave shapes that the oscillator can generate">Figure 4-4</a>.</p>
<div class="figure" id="fig21">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0404.png" alt="Types of basic soundwave shapes that the oscillator can generate"></div></div>
<div class="figure-title">Figure 4-4. Types of basic soundwave shapes that the oscillator can
      generate</div>
</div>
<p id="oscillators_can">Oscillators can easily be used in audio graphs in place of
    <code class="code">AudioBufferSourceNodes</code>. An example of this follows:</p>
<pre class="programlisting" data-language="javascript" id="function_plays"><code class="kd">function</code> <code class="nx">play</code><code class="p">(</code><code class="nx">semitone</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Create some sweet sweet nodes.</code>
  <code class="kd">var</code> <code class="nx">oscillator</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createOscillator</code><code class="p">();</code>
  <code class="nx">oscillator</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
  <code class="c1">// Play a sine type curve at A4 frequency (440hz).</code>
  <code class="nx">oscillator</code><code class="p">.</code><code class="nx">frequency</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">440</code><code class="p">;</code>
  <code class="nx">oscillator</code><code class="p">.</code><code class="nx">detune</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="nx">semitone</code> <code class="o">*</code> <code class="mi">100</code><code class="p">;</code>
  <code class="c1">// Note: this constant will be replaced with "sine".</code>
  <code class="nx">oscillator</code><code class="p">.</code><code class="nx">type</code> <code class="o">=</code> <code class="nx">oscillator</code><code class="p">.</code><code class="nx">SINE</code><code class="p">;</code>
  <code class="nx">oscillator</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="para_id7" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 598px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/oscillator/index.html"> </iframe></div>
<p class="online_only">
    </p>
<p id="in_addition_to__id2">In addition to these basic wave types, you can create a custom wave
    table for your oscillator by using harmonic tables. This lets you
    efficiently create wave shapes that are much more complex than the
    previous ones. This topic is very important for musical synthesis
    applications, but is outside of the scope of this book.</p>
</div></section><section class="chapter" data-original-filename="ch05.xml" id="ch05"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Analysis and Visualization</h1></div></div></div>
<p id="so_far_weve_on">So far we’ve only talked about audio synthesis and processing, but
  that is only half of the functionality that the Web Audio API provides. The
  other half, audio analysis, is all about understanding what the sound that
  is being played is like. The canonical example of this feature is
  visualization, but there are many other applications far outside the scope
  of this book, including pitch detection, rhythm detection, and speech
  recognition.</p>
<p id="this_is_an_impo">This is an important topic for us as game developers and interactive
  application builders for a couple of reasons. Firstly, a good visual
  analyzer can act as a sort of debugging tool (obviously in addition to your
  ears and a good metering setup) for tweaking sounds to be just right.
  Secondly, visualization is critical for any games and applications related
  to music, from games like <span class="emphasis"><em>Guitar Hero</em></span> to software like
  GarageBand.</p>
<div class="sect1" data-original-filename="ch05.xml" id="s05_1">
<div class="titlepage"><div><div><h2 class="title">Frequency Analysis</h2></div></div></div>
<p id="the_main_way_of">The main way of doing sound analysis with the Web Audio API is to
    use <code class="code">AnalyserNodes</code>. These nodes do not change the sound in any
    way, and can be placed anywhere in your audio context. Once this node is
    in your graph, it provides two main ways for you to inspect the sound
    wave: over the time domain and over the frequency domain.</p>
<p id="the_results_you">The results you get are based on FFT analysis over a certain buffer
    size. We have a few knobs to customize the output of the node:</p>
<div class="variablelist" id="fftsizethis_def"><dl class="variablelist">
<dt><span class="term"><code class="literal">fftSize</code></span></dt>
<dd><p id="this_defines_th">This defines the buffer size that is used to perform the
          analysis. It must be a power of two. Higher values will result in
          more fine-grained analysis of the signal, at the cost of some
          performance loss.</p></dd>
<dt><span class="term"><code class="literal">frequencyBinCount</code></span></dt>
<dd><p id="this_is_a_read">This is a read-only property, set automatically as
          <code class="literal">fftSize</code>/2.</p></dd>
<dt><span class="term"><code class="literal">smoothingTimeConstant</code></span></dt>
<dd><p id="this_is_a_value">This is a value between zero and one. A value of one causes a
          large moving average window and smoothed results. A value of zero
          means no moving average, and quickly fluctuating results.</p></dd>
</dl></div>
<p id="the_basic_setup">The basic setup is to insert the analyzer node into the interesting
    part of our audio graph:</p>
<pre class="programlisting" data-language="javascript" id="assume_that_"><code class="c1">// Assume that node A is ordinarily connected to B.</code>
<code class="kd">var</code> <code class="nx">analyser</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createAnalyser</code><code class="p">();</code>
<code class="nx">A</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">analyser</code><code class="p">);</code>
<code class="nx">analyser</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">B</code><code class="p">);</code></pre>
<p id="then_we_can_get">Then we can get frequency or time domain arrays as follows:</p>
<pre class="programlisting" data-language="javascript" id="var_freqdomain__id1"><code class="kd">var</code> <code class="nx">freqDomain</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Float32Array</code><code class="p">(</code><code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">);</code>
<code class="nx">analyser</code><code class="p">.</code><code class="nx">getFloatFrequencyData</code><code class="p">(</code><code class="nx">freqDomain</code><code class="p">);</code></pre>
<p id="in_the_previous_id2">In the previous example, <code class="literal">freqDomain</code> is an array of 32-bit floats
    corresponding to the frequency domain. These values are normalized to be
    between zero and one. The indexes of the output can be mapped linearly
    between zero and the <span class="emphasis"><em>nyquist frequency</em></span>, which is
    defined to be half of the sampling rate (available in the Web Audio API
    via <code class="code">context.sampleRate</code>). The following snippet maps from
    frequency to the correct bucket in the array of frequencies:</p>
<pre class="programlisting" data-language="javascript" id="function_getfre"><code class="kd">function</code> <code class="nx">getFrequencyValue</code><code class="p">(</code><code class="nx">frequency</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">nyquist</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">sampleRate</code><code class="o">/</code><code class="mi">2</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">index</code> <code class="o">=</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">round</code><code class="p">(</code><code class="nx">frequency</code><code class="o">/</code><code class="nx">nyquist</code> <code class="o">*</code> <code class="nx">freqDomain</code><code class="p">.</code><code class="nx">length</code><code class="p">);</code>
  <code class="k">return</code> <code class="nx">freqDomain</code><code class="p">[</code><code class="nx">index</code><code class="p">];</code>
<code class="p">}</code></pre>
<p id="if_we_are_analy">If we are analyzing a 1,000-Hz sine wave, for example, we would
    expect that <code class="literal">getFrequencyValue(1000)</code>
    would return a peak value in the graph, as shown in <a class="xref" href="ch05.html#fig22" title="Figure 5-1. A 1,000-Hz tone being visualized (the full domain extends from 0 to 22,050 Hz)">Figure 5-1</a>.</p>
<p id="the_frequency_d">The frequency domain is also available in 8-bit unsigned units via
    the <code class="code">getByteFrequencyData</code> call. The values of these integers
    is scaled to fit between <code class="code">minDecibels</code> and
    <code class="code">maxDecibels</code> (in dBFS) properties on the analyzer node, so
    these parameters can be tweaked to scale the output as desired.</p>
<div class="figure" id="fig22">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0501.png" alt="A 1,000-Hz tone being visualized (the full domain extends from 0 to 22,050 Hz)"></div></div>
<div class="figure-title">Figure 5-1. A 1,000-Hz tone being visualized (the full domain extends from 0
      to 22,050 Hz)</div>
</div>
</div>
<div class="sect1" data-original-filename="ch05.xml" id="s05_2">
<div class="titlepage"><div><div><h2 class="title">Animating with requestAnimationFrame</h2></div></div></div>
<p id="if_we_want_to_b">If we want to build a visualization for our soundform, we need to
    periodically query the analyzer, process the results, and render them. We
    can do this by setting up a JavaScript timer like <code class="code">setInterval</code>
    or <code class="code">setTimeout</code>, but there’s a better way:
    <code class="code">requestAnimationFrame</code>. This API lets the browser incorporate
    your custom draw function into its native rendering loop, which is a great
    performance improvement. Instead of forcing it to draw at specific
    intervals and contending with the rest of the things a browser does, you
    just request it to be placed in the queue, and the browser will get to it
    as quickly as it can.</p>
<p id="because_the_req">Because the <code class="code">requestAnimationFrame</code> API is still
    experimental, we need to use the prefixed version depending on user agent,
    and fall back to a rough equivalent: <code class="code">setTimeout</code>. The code for
    this is as follows:</p>
<pre class="programlisting" data-language="javascript" id="windowrequesta"><code class="nb">window</code><code class="p">.</code><code class="nx">requestAnimationFrame</code> <code class="o">=</code> <code class="p">(</code><code class="kd">function</code><code class="p">(){</code>
<code class="k">return</code> <code class="nb">window</code><code class="p">.</code><code class="nx">requestAnimationFrame</code>  <code class="o">||</code>
  <code class="nb">window</code><code class="p">.</code><code class="nx">webkitRequestAnimationFrame</code> <code class="o">||</code>
  <code class="nb">window</code><code class="p">.</code><code class="nx">mozRequestAnimationFrame</code>    <code class="o">||</code>
  <code class="nb">window</code><code class="p">.</code><code class="nx">oRequestAnimationFrame</code>      <code class="o">||</code>
  <code class="nb">window</code><code class="p">.</code><code class="nx">msRequestAnimationFrame</code>     <code class="o">||</code>
  <code class="kd">function</code><code class="p">(</code><code class="nx">callback</code><code class="p">){</code>
  <code class="nb">window</code><code class="p">.</code><code class="nx">setTimeout</code><code class="p">(</code><code class="nx">callback</code><code class="p">,</code> <code class="mi">1000</code> <code class="o">/</code> <code class="mi">60</code><code class="p">);</code>
<code class="p">};</code>
<code class="p">})();</code></pre>
<p id="once_we_have_th">Once we have this <code class="literal">requestAnimationFrame</code> function defined, we
    should use it to query the analyzer node to give us detailed information
    about the state of the audio stream.</p>
</div>
<div class="sect1" data-original-filename="ch05.xml" id="s05_3">
<div class="titlepage"><div><div><h2 class="title">Visualizing Sound</h2></div></div></div>
<p id="putting_it_all_">Putting it all together, we can set up a render loop that queries
    and renders the analyzer for its current frequency analysis as before,
    into a <code class="literal">freqDomain</code> array:</p>
<pre class="programlisting" data-language="javascript" id="var_freqdomain__id2"><code class="kd">var</code> <code class="nx">freqDomain</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Uint8Array</code><code class="p">(</code><code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">);</code>
<code class="nx">analyser</code><code class="p">.</code><code class="nx">getByteFrequencyData</code><code class="p">(</code><code class="nx">freqDomain</code><code class="p">);</code>
<code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">value</code> <code class="o">=</code> <code class="nx">freqDomain</code><code class="p">[</code><code class="nx">i</code><code class="p">];</code>
  <code class="kd">var</code> <code class="nx">percent</code> <code class="o">=</code> <code class="nx">value</code> <code class="o">/</code> <code class="mi">256</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">height</code> <code class="o">=</code> <code class="nx">HEIGHT</code> <code class="o">*</code> <code class="nx">percent</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">offset</code> <code class="o">=</code> <code class="nx">HEIGHT</code> <code class="o">-</code> <code class="nx">height</code> <code class="o">-</code> <code class="mi">1</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">barWidth</code> <code class="o">=</code> <code class="nx">WIDTH</code><code class="o">/</code><code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">hue</code> <code class="o">=</code> <code class="nx">i</code><code class="o">/</code><code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code> <code class="o">*</code> <code class="mi">360</code><code class="p">;</code>
  <code class="nx">drawContext</code><code class="p">.</code><code class="nx">fillStyle</code> <code class="o">=</code> <code class="s1">'hsl('</code> <code class="o">+</code> <code class="nx">hue</code> <code class="o">+</code> <code class="s1">', 100%, 50%)'</code><code class="p">;</code>
  <code class="nx">drawContext</code><code class="p">.</code><code class="nx">fillRect</code><code class="p">(</code><code class="nx">i</code> <code class="o">*</code> <code class="nx">barWidth</code><code class="p">,</code> <code class="nx">offset</code><code class="p">,</code> <code class="nx">barWidth</code><code class="p">,</code> <code class="nx">height</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="we_can_do_a_sim">We can do a similar thing for the time-domain data as well:</p>
<pre class="programlisting" data-language="javascript" id="var_timedomain_"><code class="kd">var</code> <code class="nx">timeDomain</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Uint8Array</code><code class="p">(</code><code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">);</code>
<code class="nx">analyser</code><code class="p">.</code><code class="nx">getByteTimeDomainData</code><code class="p">(</code><code class="nx">freqDomain</code><code class="p">);</code>
<code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">value</code> <code class="o">=</code> <code class="nx">timeDomain</code><code class="p">[</code><code class="nx">i</code><code class="p">];</code>
  <code class="kd">var</code> <code class="nx">percent</code> <code class="o">=</code> <code class="nx">value</code> <code class="o">/</code> <code class="mi">256</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">height</code> <code class="o">=</code> <code class="nx">HEIGHT</code> <code class="o">*</code> <code class="nx">percent</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">offset</code> <code class="o">=</code> <code class="nx">HEIGHT</code> <code class="o">-</code> <code class="nx">height</code> <code class="o">-</code> <code class="mi">1</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">barWidth</code> <code class="o">=</code> <code class="nx">WIDTH</code><code class="o">/</code><code class="nx">analyser</code><code class="p">.</code><code class="nx">frequencyBinCount</code><code class="p">;</code>
  <code class="nx">drawContext</code><code class="p">.</code><code class="nx">fillStyle</code> <code class="o">=</code> <code class="s1">'black'</code><code class="p">;</code>
  <code class="nx">drawContext</code><code class="p">.</code><code class="nx">fillRect</code><code class="p">(</code><code class="nx">i</code> <code class="o">*</code> <code class="nx">barWidth</code><code class="p">,</code> <code class="nx">offset</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="this_code_plots">This code plots time-domain values using HTML5 canvas, creating a
    simple visualizer that renders a graph of the waveform on top of the
    colorful bar graph, which represents frequency-domain data. The result is
    a canvas output that looks like <a class="xref" href="ch05.html#fig23" title="Figure 5-2. A screenshot of a visualizer in action">Figure 5-2</a>, and changes with
    time.</p>
<div class="figure" id="fig23">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0502.png" alt="A screenshot of a visualizer in action"></div></div>
<div class="figure-title">Figure 5-2. A screenshot of a visualizer in action</div>
</div>
<p id="para_id8" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 800px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/visualizer/index.html"> </iframe></div>
<p class="online_only">
    </p>
<p id="our_approach_to">Our approach to visualization misses a lot of data. For music
    visualization purposes, that’s fine. If, however, we want to perform a
    comprehensive analysis of the whole audio buffer, we should look to other
    methods.</p>
</div></section><section class="chapter" data-original-filename="ch06.xml" id="ch06"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Advanced Topics</h1></div></div></div>
<p id="this_chapter_co">This chapter covers topics that are very important, but slightly more
  complex than the rest of the book. We will dive into adding effects to
  sounds, generating synthetic sound effects without any audio buffers at all,
  simulating effects of different acoustic environments, and spatializing
  sound in 3D space.</p>
<div class="sidebar theory" id="s06_1">
<div class="titlepage"><div><div><div class="sidebar-title">Biquad Filters</div></div></div></div>
<p id="a_filter_can_em">A filter can emphasize or de-emphasize certain parts of the
    frequency spectrum of a sound. Visually, it can be shown as a graph over
    the frequency domain called a <span class="emphasis"><em>frequency response
    graph</em></span> (see <a class="xref" href="ch06.html#fig24" title="Figure 6-1. Frequency response graph for a low-pass filter">Figure 6-1</a>). For each frequency, the
    higher the value of the graph, the more emphasis is placed on that part of
    the frequency range. A graph sloping downward places more emphasis on low
    frequencies and less on high frequencies.</p>
<p id="web_audio_filte">Web Audio filters can be configured with three parameters: gain,
    frequency, and a quality factor (also known as Q). These parameters all
    affect the frequency response graph differently.</p>
<p id="there_are_many_">There are many kinds of filters that can be used to achieve certain
    kinds of effects:</p>
<div class="variablelist" id="lowpass_filter"><dl class="variablelist">
<dt><span class="term">Low-pass filter</span></dt>
<dd><p id="makes_sounds_mo_id1">Makes sounds more muffled</p></dd>
<dt><span class="term">High-pass filter</span></dt>
<dd><p id="makes_sounds_mo_id2">Makes sounds more tinny</p></dd>
<dt><span class="term">Band-pass filter</span></dt>
<dd><p id="cuts_off_lows_a">Cuts off lows and highs (e.g., telephone filter)</p></dd>
<dt><span class="term">Low-shelf filter</span></dt>
<dd><p id="affects_the_amo_id1">Affects the amount of bass in a sound (like the bass knob on a
          stereo)</p></dd>
<dt><span class="term">High-shelf filter</span></dt>
<dd><p id="affects_the_amo_id2">Affects the amount of treble in a sound (like the treble knob
          on a stereo)</p></dd>
<dt><span class="term">Peaking filter</span></dt>
<dd><p id="affects_the_amo_id3">Affects the amount of midrange in a sound (like the mid knob
          on a stereo)</p></dd>
<dt><span class="term">Notch filter</span></dt>
<dd><p id="removes_unwante">Removes unwanted sounds in a narrow frequency range</p></dd>
<dt><span class="term">All-pass filter</span></dt>
<dd><p id="creates_phaser_">Creates phaser effects</p></dd>
</dl></div>
<div class="figure" id="fig24">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0601.png" alt="Frequency response graph for a low-pass filter"></div></div>
<div class="figure-title">Figure 6-1. Frequency response graph for a low-pass filter</div>
</div>
<p id="all_of_these_bi">All of these biquad filters stem from a common mathematical model
    and can all be graphed like the low-pass filter in <a class="xref" href="ch06.html#fig24" title="Figure 6-1. Frequency response graph for a low-pass filter">Figure 6-1</a>. More details about these filters can be found in more
    mathematically demanding books such as <span class="emphasis"><em>Real Sound Synthesis for
    Interactive Applications</em></span> by Perry R. Cook (A K Peters, 2002),
    which I highly recommend reading if you are interested in audio
    fundamentals.</p>
</div>
<div class="sect1" data-original-filename="ch06.xml" id="s06_2">
<div class="titlepage"><div><div><h2 class="title">Adding Effects via Filters</h2></div></div></div>
<p id="using_the_web_a">Using the Web Audio API, we can apply the filters discussed above
    using <code class="code">BiquadFilterNodes</code>. This type of audio node is very
    commonly used to build equalizers and manipulate sounds in interesting
    ways. Let’s set up a simple low-pass filter to eliminate low frequency
    noise from a sound sample:</p>
<pre class="programlisting" data-language="javascript" id="create_a_fil"><code class="c1">// Create a filter</code>
<code class="kd">var</code> <code class="nx">filter</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBiquadFilter</code><code class="p">();</code>
<code class="c1">// Note: the Web Audio spec is moving from constants to strings.</code>
<code class="c1">// filter.type = 'lowpass';</code>
<code class="nx">filter</code><code class="p">.</code><code class="nx">type</code> <code class="o">=</code> <code class="nx">filter</code><code class="p">.</code><code class="nx">LOWPASS</code><code class="p">;</code>
<code class="nx">filter</code><code class="p">.</code><code class="nx">frequency</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">100</code><code class="p">;</code>
<code class="c1">// Connect the source to it, and the filter to the destination.</code></pre>
<p id="para_id9" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 396px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/filter/index.html"> </iframe></div>
<p class="online_only">
    </p>
<p id="the_biquadfilte">The <code class="code">BiquadFilterNode</code> has support for all of the
    commonly used second-order filter types. We can configure these nodes with
    the same parameters as discussed in the previous section, and also
    visualize the frequency response graphs by using the
    <code class="code">getFrequencyResponse</code> method on the node. Given an array of
    frequencies, this function returns an array of magnitudes of responses
    corresponding to each frequency.</p>
<p id="chris_wilson_an">Chris Wilson and Chris Rogers put together a great visualizer sample
    (<a class="xref" href="ch06.html#fig25" title="Figure 6-2. A graph of the frequency response of a low-pass filter with parameters">Figure 6-2</a>) that shows the frequency responses of all of
    the filter types available in the Web Audio API.</p>
<div class="figure" id="fig25">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0602.png" alt="A graph of the frequency response of a low-pass filter with parameters"></div></div>
<div class="figure-title">Figure 6-2. A graph of the frequency response of a low-pass filter with
      parameters</div>
</div>
<p id="para_id10" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 729px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/frequency-response/index.html"> </iframe></div>
<p class="online_only">
    </p>
</div>
<div class="sect1" data-original-filename="ch06.xml" id="s06_3">
<div class="titlepage"><div><div><h2 class="title">Procedurally Generated Sound</h2></div></div></div>
<p id="up_to_now_we_h">Up to now, we have been assuming that your game’s sound sources are
    static. An audio designer creates a bunch of assets and hands them over to
    you. Then, you play them back with some parameterization depending on
    local conditions (for example, the room ambiance and relative positions of
    sources and listeners). This approach has a few disadvantages:</p>
<div class="orderedlist" id="sound_assets_wi_id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="sound_assets_wi_id2">Sound assets will be very large. This is especially bad on the
        Web, where instead of loading from a hard drive, you load from a
        network (at least the first time), which is roughly an order of
        magnitude slower.</p></li>
<li class="listitem"><p id="even_with_many_">Even with many assets and tweaks to each, there is limited
        variety.</p></li>
<li class="listitem"><p id="you_need_to_fin">You need to find assets by scouring sound effects libraries, and
        then maybe worry about royalties. Plus, chances are, any given sound
        effect is already being used in other applications, so your users have
        unintended associations.</p></li>
</ol></div>
<p id="we_can_use_the_">We can use the Web Audio API to fully generate sound procedurally.
    For example, let’s simulate a gun firing. We begin with a buffer of white
    noise, which we can generate with a <code class="code">ScriptProcessorNode</code> as
    follows:</p>
<pre class="programlisting" data-language="javascript" id="function_whiten_id1"><code class="kd">function</code> <code class="nx">WhiteNoiseScript</code><code class="p">()</code> <code class="p">{</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createScriptProcessor</code><code class="p">(</code><code class="mi">1024</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">);</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">onaudioprocess</code> <code class="o">=</code> <code class="k">this</code><code class="p">.</code><code class="nx">process</code><code class="p">;</code>
<code class="p">}</code>

<code class="nx">WhiteNoiseScript</code><code class="p">.</code><code class="nx">prototype</code><code class="p">.</code><code class="nx">process</code> <code class="o">=</code> <code class="kd">function</code><code class="p">(</code><code class="nx">e</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">L</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">outputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">R</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">outputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">1</code><code class="p">);</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">L</code><code class="p">.</code><code class="nx">length</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">L</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="p">((</code><code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code><code class="p">);</code>
    <code class="nx">R</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="nx">L</code><code class="p">[</code><code class="nx">i</code><code class="p">];</code>
  <code class="p">}</code>
<code class="p">};</code></pre>
<p id="for_more_inform_id2">For more information on <code class="literal">ScriptProcessorNode</code>s, see
    <a class="xref" href="ch06.html#s06_6" title="Audio Processing with JavaScript">“Audio Processing with JavaScript”</a>.</p>
<p id="this_code_is_no">This code is not an efficient implementation because JavaScript is
    required to constantly and dynamically create a stream of white noise. To
    increase efficiency, we can programmatically generate a mono <code class="literal">AudioBuffer</code> of white noise as follows:</p>
<pre class="programlisting" data-language="javascript" id="function_whiten_id2"><code class="kd">function</code> <code class="nx">WhiteNoiseGenerated</code><code class="p">(</code><code class="nx">callback</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Generate a 5 second white noise buffer.</code>
  <code class="kd">var</code> <code class="nx">lengthInSamples</code> <code class="o">=</code> <code class="mi">5</code> <code class="o">*</code> <code class="nx">context</code><code class="p">.</code><code class="nx">sampleRate</code><code class="p">;</code>
  <code class="kd">var</code> <code class="nx">buffer</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBuffer</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nx">lengthInSamples</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">sampleRate</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">data</code> <code class="o">=</code> <code class="nx">buffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>

  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">lengthInSamples</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">data</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="p">((</code><code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()</code> <code class="o">*</code> <code class="mi">2</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code><code class="p">);</code>
  <code class="p">}</code>

  <code class="c1">// Create a source node from the buffer.</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="nx">buffer</code><code class="p">;</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">loop</code> <code class="o">=</code> <code class="kc">true</code><code class="p">;</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="next_we_can_si">Next, we can simulate various phases of the gun firing—attack,
    decay, and release—in an envelope:</p>
<pre class="programlisting" data-language="javascript" id="function_envelo"><code class="kd">function</code> <code class="nx">Envelope</code><code class="p">()</code> <code class="p">{</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createGain</code><code class="p">()</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code>
<code class="p">}</code>

<code class="nx">Envelope</code><code class="p">.</code><code class="nx">prototype</code><code class="p">.</code><code class="nx">addEventToQueue</code> <code class="o">=</code> <code class="kd">function</code><code class="p">()</code> <code class="p">{</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code><code class="p">);</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code> <code class="o">+</code> <code class="mf">0.001</code><code class="p">);</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mf">0.3</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code> <code class="o">+</code> <code class="mf">0.101</code><code class="p">);</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">node</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">linearRampToValueAtTime</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nx">context</code><code class="p">.</code><code class="nx">currentTime</code> <code class="o">+</code> <code class="mf">0.500</code><code class="p">);</code>
<code class="p">};</code></pre>
<p id="finally_we_can">Finally, we can connect the voice outputs to a filter to allow a
    simulation of distance:</p>
<pre class="programlisting" data-language="javascript" id="thisvoices__">  <code class="k">this</code><code class="p">.</code><code class="nx">voices</code> <code class="o">=</code> <code class="p">[];</code>
  <code class="k">this</code><code class="p">.</code><code class="nx">voiceIndex</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code>

  <code class="kd">var</code> <code class="nx">noise</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">WhiteNoise</code><code class="p">();</code>

  <code class="kd">var</code> <code class="nx">filter</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBiquadFilter</code><code class="p">();</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">type</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">Q</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">1</code><code class="p">;</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">frequency</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">800</code><code class="p">;</code>

  <code class="c1">// Initialize multiple voices.</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">VOICE_COUNT</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="kd">var</code> <code class="nx">voice</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Envelope</code><code class="p">();</code>
    <code class="nx">noise</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">voice</code><code class="p">.</code><code class="nx">node</code><code class="p">);</code>
    <code class="nx">voice</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">filter</code><code class="p">);</code>
    <code class="k">this</code><code class="p">.</code><code class="nx">voices</code><code class="p">.</code><code class="nx">push</code><code class="p">(</code><code class="nx">voice</code><code class="p">);</code>
  <code class="p">}</code>

  <code class="kd">var</code> <code class="nx">gainMaster</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createGainNode</code><code class="p">();</code>
  <code class="nx">gainMaster</code><code class="p">.</code><code class="nx">gain</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">5</code><code class="p">;</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">gainMaster</code><code class="p">);</code>

  <code class="nx">gainMaster</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code></pre>
<p id="this_example_is">This example is borrowed from BBC’s <a class="ulink" href="http://webaudio.prototyping.bbc.co.uk/gunfire/" target="_top">gunfire effects
    page</a> with small modifications, including a port to
    JavaScript.</p>
<p id="para_id11" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 310px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/procedural/index.html"> </iframe></div>
<p class="online_only">
    </p>
<p id="as_you_can_see">As you can see, this approach is very powerful but gets complicated
    pretty quickly, going beyond the scope of this book. For more information
    about procedural sound generation, take a look at <a class="ulink" href="http://obiwannabe.co.uk/tutorials/html/tutorials_main.html" target="_top">Andy
    Farnell’s Practical Synthetic Sound Design</a> tutorials and
    book.</p>
</div>
<div class="sect1" data-original-filename="ch06.xml" id="s06_4">
<div class="titlepage"><div><div><h2 class="title">Room Effects</h2></div></div></div>
<p id="before_sound_ge">Before sound gets from its source to our ears, it bounces off walls,
    buildings, furniture, carpets, and other objects. Every such collision
    changes properties of the sound. For example, clapping your hands outside
    sounds very different from clapping your hands inside a large cathedral,
    which can cause audible reverberations for several seconds. Games with
    high production value aim to imitate these effects. Creating a separate
    set of samples for each acoustic environment is often prohibitively
    expensive, since it requires a lot of effort from the audio designer, and
    a lot of assets, and thus a larger amount of game data.</p>
<p id="the_web_audio_a_id7">The Web Audio API comes with a facility to simulate these various
    acoustic environments called a <code class="literal">ConvolverNode</code>. Examples of effects that you can
    get out of the convolution engine include chorus effects, reverberation,
    and telephone-like speech.</p>
<p id="the_idea_for_pr">The idea for producing room effects is to play back a reference
    sound in a room, record it, and then (metaphorically) take the difference
    between the original sound and the recorded one. The result of this is an
    impulse response that captures the effect that the room has on a sound.
    These impulse responses are painstakingly recorded in very specific studio
    settings, and doing this on your own requires serious dedication. Luckily,
    there are sites that host many of these pre-recorded impulse response
    files (stored as audio files) for your convenience.</p>
<p id="the_web_audio_a_id8">The Web Audio API provides an easy way to apply these impulse
    responses to your sounds using the <code class="literal">ConvolverNode</code>. This node takes an impulse
    response buffer, which is a regular <code class="literal">AudioBuffer</code> with the impulse response file
    loaded into it. The convolver is effectively a very complex filter (like
    the <code class="literal">BiquadFilterNode</code>), but rather than
    selecting from a set of effect types, it can be configured with an
    arbitrary filter response:</p>
<pre class="programlisting" data-language="javascript" id="var_impulseresp"><code class="kd">var</code> <code class="nx">impulseResponseBuffer</code> <code class="o">=</code> <code class="kc">null</code><code class="p">;</code>
<code class="kd">function</code> <code class="nx">loadImpulseResponse</code><code class="p">()</code> <code class="p">{</code>
  <code class="nx">loadBuffer</code><code class="p">(</code><code class="s1">'impulse.wav'</code><code class="p">,</code> <code class="kd">function</code><code class="p">(</code><code class="nx">buffer</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">impulseResponseBuffer</code> <code class="o">=</code> <code class="nx">buffer</code><code class="p">;</code>
  <code class="p">});</code>
<code class="p">}</code>

<code class="kd">function</code> <code class="nx">play</code><code class="p">()</code> <code class="p">{</code>
  <code class="c1">// Make a source node for the sample.</code>
  <code class="kd">var</code> <code class="nx">source</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBufferSource</code><code class="p">();</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="k">this</code><code class="p">.</code><code class="nx">buffer</code><code class="p">;</code>
  <code class="c1">// Make a convolver node for the impulse response.</code>
  <code class="kd">var</code> <code class="nx">convolver</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createConvolver</code><code class="p">();</code>
  <code class="c1">// Set the impulse response buffer.</code>
  <code class="nx">convolver</code><code class="p">.</code><code class="nx">buffer</code> <code class="o">=</code> <code class="nx">impulseResponseBuffer</code><code class="p">;</code>
  <code class="c1">// Connect graph.</code>
  <code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">convolver</code><code class="p">);</code>
  <code class="nx">convolver</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
<code class="p">}</code></pre>
<p id="the_convolver_n">The convolver node “smushes” the input sound and its impulse
    response by computing a convolution, a mathematically intensive function.
    The result is something that sounds as if it was produced in the room
    where the impulse response was recorded. In practice, it often makes sense
    to mix the original sound (called the <span class="emphasis"><em>dry mix</em></span>) with
    the convolved sound (called the <span class="emphasis"><em>wet mix</em></span>), and use an
    equal-power crossfade to control how much of the effect you want to
    apply.</p>
<p id="its_also_possi">It’s also possible to generate these impulse responses
    synthetically, but this topic is outside of the scope of this book.</p>
<p id="para_id12" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 294px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/room-effects/index.html"> </iframe></div>
<p class="online_only">
    </p>
</div>
<div class="sect1" data-original-filename="ch06.xml" id="s06_5">
<div class="titlepage"><div><div><h2 class="title">Spatialized Sound</h2></div></div></div>
<p id="games_are_often">Games are often set in a world where objects have positions in
    space, either in 2D or in 3D. If this is the case, spatialized audio can
    greatly increase the immersiveness of the experience. Luckily, the Web
    Audio API comes with built-in positional audio features (stereo for now)
    that are quite straightforward to use.</p>
<p id="as_you_experime">As you <a class="ulink" href="http://connect.creativelabs.com/openal/default.aspx" target="_top">experiment with
    spatialized sound</a>, make sure that you are listening through stereo
    speakers (preferably headphones). This will give you a better idea of how
    the left and right channels are being transformed by your spatialization
    approach.</p>
<p id="the_web_audio_a_id9">The Web Audio API model has three aspects of increasing complexity,
    with many concepts borrowed from OpenAL:</p>
<div class="orderedlist" id="position_and_or_id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="position_and_or_id2">Position and orientation of sources and listeners</p></li>
<li class="listitem"><p id="parameters_asso">Parameters associated with the source audio cones</p></li>
<li class="listitem"><p id="relative_veloci">Relative velocities of sources and listeners</p></li>
</ol></div>
<p id="there_is_a_sing">There is a single listener (<code class="literal">AudioListener</code>) attached to the Web Audio API
    context that can be configured in space through position and orientation.
    Each source can be passed through a panner node (<code class="literal">AudioPannerNode</code>), which spatializes the input
    audio. Based on the relative position of the sources and the listener, the
    Web Audio API computes the correct gain modifications.</p>
<p id="there_are_a_few">There are a few things to know about the assumptions that the API
    makes. The first is that the listener is at the origin (0, 0, 0) by
    default. Positional API coordinates are unitless, so in practice, it takes
    some multiplier tweaking to make things sound the way you want. Secondly,
    orientations are specified as direction vectors (with a length of one).
    Finally, in this coordinate space, positive <span class="emphasis"><em>y</em></span> points
    upward, which is the opposite of most computer graphics systems.</p>
<p id="with_these_thin">With these things in mind, here’s an example of how you can change
    the position of a source node that is being spatialized in 2D via a panner
    node (<code class="literal">PannerNode</code>):</p>
<pre class="programlisting" data-language="javascript" id="position_the"><code class="c1">// Position the listener at the origin (the default, just added for the sake of being explicit)</code>
<code class="nx">context</code><code class="p">.</code><code class="nx">listener</code><code class="p">.</code><code class="nx">setPosition</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">);</code>

<code class="c1">// Position the panner node.</code>
<code class="c1">// Assume X and Y are in screen coordinates and the listener is at screen center.</code>
<code class="kd">var</code> <code class="nx">panner</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createPanner</code><code class="p">();</code>
<code class="kd">var</code> <code class="nx">centerX</code> <code class="o">=</code> <code class="nx">WIDTH</code><code class="o">/</code><code class="mi">2</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">centerY</code> <code class="o">=</code> <code class="nx">HEIGHT</code><code class="o">/</code><code class="mi">2</code><code class="p">;</code>
<code class="kd">var</code> <code class="nx">x</code> <code class="o">=</code> <code class="p">(</code><code class="nx">X</code> <code class="o">-</code> <code class="nx">centerX</code><code class="p">)</code>  <code class="o">/</code> <code class="nx">WIDTH</code><code class="p">;</code>
<code class="c1">// The y coordinate is flipped to match the canvas coordinate space.</code>
<code class="kd">var</code> <code class="nx">y</code> <code class="o">=</code> <code class="p">(</code><code class="nx">Y</code> <code class="o">-</code> <code class="nx">centerY</code><code class="p">)</code> <code class="o">/</code> <code class="nx">HEIGHT</code><code class="p">;</code>
<code class="c1">// Place the z coordinate slightly in behind the listener.</code>
<code class="kd">var</code> <code class="nx">z</code> <code class="o">=</code> <code class="o">-</code><code class="mf">0.5</code><code class="p">;</code>
<code class="c1">// Tweak multiplier as necessary.</code>
<code class="kd">var</code> <code class="nx">scaleFactor</code> <code class="o">=</code> <code class="mi">2</code><code class="p">;</code>
<code class="nx">panner</code><code class="p">.</code><code class="nx">setPosition</code><code class="p">(</code><code class="nx">x</code> <code class="o">*</code> <code class="nx">scaleFactor</code><code class="p">,</code> <code class="nx">y</code> <code class="o">*</code> <code class="nx">scaleFactor</code><code class="p">,</code> <code class="nx">z</code><code class="p">);</code>

<code class="c1">// Convert angle into a unit vector.</code>
<code class="nx">panner</code><code class="p">.</code><code class="nx">setOrientation</code><code class="p">(</code><code class="nb">Math</code><code class="p">.</code><code class="nx">cos</code><code class="p">(</code><code class="nx">angle</code><code class="p">),</code> <code class="o">-</code><code class="nb">Math</code><code class="p">.</code><code class="nx">sin</code><code class="p">(</code><code class="nx">angle</code><code class="p">),</code> <code class="mi">1</code><code class="p">);</code>

<code class="c1">// Connect the node you want to spatialize to a panner.</code>
<code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">panner</code><code class="p">);</code></pre>
<p id="in_addition_to__id3">In addition to taking into account relative positions and
    orientations, each source has a configurable audio cone, as shown in <a class="xref" href="ch06.html#fig26" title="Figure 6-3. A diagram of panners and the listener in 2D space">Figure 6-3</a>.</p>
<div class="figure" id="fig26">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0603.png" alt="A diagram of panners and the listener in 2D space"></div></div>
<div class="figure-title">Figure 6-3. A diagram of panners and the listener in 2D space</div>
</div>
<p id="once_you_have_s">Once you have specified an inner and outer cone, you end up with a
    separation of space into three parts, as seen in <a class="xref" href="ch06.html#fig26" title="Figure 6-3. A diagram of panners and the listener in 2D space">Figure 6-3</a>:</p>
<div class="orderedlist" id="inner_coneouter"><ol class="orderedlist" type="1">
<li class="listitem"><p id="inner_cone">Inner cone</p></li>
<li class="listitem"><p id="outer_cone">Outer cone</p></li>
<li class="listitem"><p id="neither_cone">Neither cone</p></li>
</ol></div>
<p id="each_of_these_s">Each of these sub-spaces can have a gain multiplier associated with
    it as an extra hint for the positional model. For example, to emulate
    targeted sound, we might have the following configuration:</p>
<pre class="programlisting" data-language="javascript" id="pannerconeinne_id1"><code class="nx">panner</code><code class="p">.</code><code class="nx">coneInnerAngle</code> <code class="o">=</code> <code class="mi">5</code><code class="p">;</code>
<code class="nx">panner</code><code class="p">.</code><code class="nx">coneOuterAngle</code> <code class="o">=</code> <code class="mi">10</code><code class="p">;</code>
<code class="nx">panner</code><code class="p">.</code><code class="nx">coneGain</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">;</code>
<code class="nx">panner</code><code class="p">.</code><code class="nx">coneOuterGain</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">;</code></pre>
<p id="a_dispersed_sou">A dispersed sound can have a very different set of parameters. An
    omnidirectional source has a 360-degree inner cone, and its orientation
    makes no difference for <span class="keep-together">spatialization</span>:</p>
<pre class="programlisting" data-language="javascript" id="pannerconeinne_id2"><code class="nx">panner</code><code class="p">.</code><code class="nx">coneInnerAngle</code> <code class="o">=</code> <code class="mi">180</code><code class="p">;</code>
<code class="nx">panner</code><code class="p">.</code><code class="nx">coneGain</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">;</code></pre>
<p id="in_addition_to__id4">In addition to position, orientation, and sound cones, sources and
    listeners can also specify velocity. This value is important for
    simulating pitch changes as a result of the doppler effect.</p>
<p id="para_id13" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 613px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/spatialized/index.html"> </iframe></div>
<p class="online_only">
    </p>
</div>
<div class="sect1" data-original-filename="ch06.xml" id="s06_6">
<div class="titlepage"><div><div><h2 class="title">Audio Processing with JavaScript</h2></div></div></div>
<p id="generally_speak_id2">Generally speaking, the Web Audio API aims to provide enough
    primitives (mostly via audio nodes) to do most common audio tasks. The
    idea is that these modules are written in C++ and are much faster than the
    same code written in JavaScript.</p>
<p id="however_the_ap">However, the API also provides a <code class="code">ScriptProcessorNode</code>
    that lets web developers synthesize and process audio directly in
    JavaScript. For example, you could prototype custom DSP effects using this
    approach, or illustrate concepts for educational applications.</p>
<p id="to_get_started">To get started, create a <code class="literal">ScriptProcessorNode</code>.
    This node processes sound in chunks specified as a parameter to the node
    (<code class="literal">bufferSize</code>), which must be a power of two. Err on the
    side of using a larger buffer, since it gives you more of a safety margin
    against glitches if the main thread is busy with other things, such as
    page re-layout, garbage collection, or JavaScript callbacks:</p>
<pre class="programlisting" data-language="javascript" id="create_a_scr"><code class="c1">// Create a ScriptProcessorNode.</code>
<code class="kd">var</code> <code class="nx">processor</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createScriptProcessor</code><code class="p">(</code><code class="mi">2048</code><code class="p">);</code>
<code class="c1">// Assign the onProcess function to be called for every buffer.</code>
<code class="nx">processor</code><code class="p">.</code><code class="nx">onaudioprocess</code> <code class="o">=</code> <code class="nx">onProcess</code><code class="p">;</code>
<code class="c1">// Assuming source exists, connect it to a script processor.</code>
<code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">processor</code><code class="p">);</code></pre>
<p id="once_you_have_t">Once you have the audio data piping into a JavaScript function, you
    can analyze the stream by examining the input buffer, or directly change
    the output by modifying the output buffer. For example, we can easily swap
    the left and right channels by implementing the following script
    processor:</p>
<pre class="programlisting" data-language="javascript" id="function_onproc_id2"><code class="kd">function</code> <code class="nx">onProcess</code><code class="p">(</code><code class="nx">e</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">leftIn</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">inputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">rightIn</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">inputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">1</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">leftOut</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">outputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">rightOut</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">outputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">1</code><code class="p">);</code>

  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">leftIn</code><code class="p">.</code><code class="nx">length</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="c1">// Flip left and right channels.</code>
    <code class="nx">leftOut</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="nx">rightIn</code><code class="p">[</code><code class="nx">i</code><code class="p">];</code>
    <code class="nx">rightOut</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">=</code> <code class="nx">leftIn</code><code class="p">[</code><code class="nx">i</code><code class="p">];</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
<p id="note_that_you_s">Note that you should never do this channel swap in production, since
    using a <code class="literal">ChannelSplitterNode</code> followed by
    a <code class="literal">ChannelMergerNode</code> is far more
    efficient. As another example, we can add a random noise to the mix. We do
    this by simply adding a random offset to the signal. By making the signal
    completely random, we can generate white noise, which is actually quite
    useful in many applications [see <a class="xref" href="ch06.html#s06_3" title="Procedurally Generated Sound">“Procedurally Generated Sound”</a>]:</p>
<pre class="programlisting" data-language="javascript" id="function_onproc_id3"><code class="kd">function</code> <code class="nx">onProcess</code><code class="p">(</code><code class="nx">e</code><code class="p">)</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">leftOut</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">outputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">rightOut</code> <code class="o">=</code> <code class="nx">e</code><code class="p">.</code><code class="nx">outputBuffer</code><code class="p">.</code><code class="nx">getChannelData</code><code class="p">(</code><code class="mi">1</code><code class="p">);</code>

  <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">leftOut</code><code class="p">.</code><code class="nx">length</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
    <code class="c1">// Add some noise</code>
    <code class="nx">leftOut</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">+=</code> <code class="p">(</code><code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()</code> <code class="o">-</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">*</code> <code class="nx">NOISE_FACTOR</code><code class="p">;</code>
    <code class="nx">rightOut</code><code class="p">[</code><code class="nx">i</code><code class="p">]</code> <code class="o">+=</code> <code class="p">(</code><code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()</code> <code class="o">-</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">*</code> <code class="nx">NOISE_FACTOR</code><code class="p">;</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
<p id="the_main_issue_">The main issue with using script processing nodes is performance.
    Using JavaScript to implement these mathematically-intensive algorithms is
    significantly slower than implementing them directly in the native code of
    the browser.</p>
<p id="para_id14" class="online_only">
    </p>
<div class="interactive"><iframe style="width: 100%; height: 272px;" src="http://orm-other.s3.amazonaws.com/webaudioapi/samples/script-processor/index.html"> </iframe></div>
<p class="online_only">
    </p>
</div></section><section class="chapter" data-original-filename="ch07.xml" id="ch07"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Integrating with Other Technologies</h1></div></div></div>
<p id="the_web_audio_a_id10">The Web Audio API makes audio processing and analysis a fundamental
  part of the web platform. As a core building block for web developers, it is
  designed to play well with other technologies.</p>
<div class="sect1" data-original-filename="ch07.xml" id="s07_1">
<div class="titlepage"><div><div><h2 class="title">Setting Up Background Music with the &lt;audio&gt; Tag</h2></div></div></div>
<p id="as_i_mentioned_">As I mentioned at the very start of the book, the <code class="literal">&lt;audio&gt;</code> tag has many limitations that make
    it undesirable for games and interactive applications. One advantage of
    this HTML5 feature, however, is that it has built-in buffering and
    streaming support, making it ideal for long-form playback. Loading a large
    buffer is slow from a network perspective, and expensive from a
    memory-management perspective. The <code class="literal">&lt;audio&gt;</code> tag setup is ideal for music
    playback or for a game soundtrack.</p>
<p id="rather_than_goi">Rather than going the usual path of loading a sound directly by
    issuing an <code class="literal">XMLHttpRequest</code> and then
    decoding the buffer, you can use the media stream audio source node
    (<code class="literal">MediaElementAudioSourceNode</code>) to create
    nodes that behave much like audio source nodes (<code class="literal">AudioSourceNode</code>), but wrap an existing
    <code class="literal">&lt;audio&gt;</code> tag. Once we have this node connected to
    our audio graph, we can use our knowledge of the Web Audio API to do great
    things. This small example applies a low-pass filter to the <code class="literal">&lt;audio&gt;</code> tag:</p>
<pre class="programlisting" data-language="javascript" id="windowaddevent"><code class="nb">window</code><code class="p">.</code><code class="nx">addEventListener</code><code class="p">(</code><code class="s1">'load'</code><code class="p">,</code> <code class="nx">onLoad</code><code class="p">,</code> <code class="kc">false</code><code class="p">);</code>

<code class="kd">function</code> <code class="nx">onLoad</code><code class="p">()</code> <code class="p">{</code>
  <code class="kd">var</code> <code class="nx">audio</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Audio</code><code class="p">();</code>
  <code class="nx">source</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createMediaElementSource</code><code class="p">(</code><code class="nx">audio</code><code class="p">);</code>
  <code class="kd">var</code> <code class="nx">filter</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBiquadFilter</code><code class="p">();</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">type</code> <code class="o">=</code> <code class="nx">filter</code><code class="p">.</code><code class="nx">LOWPASS</code><code class="p">;</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">frequency</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mi">440</code><code class="p">;</code>

  <code class="nx">source</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="k">this</code><code class="p">.</code><code class="nx">filter</code><code class="p">);</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">context</code><code class="p">.</code><code class="nx">destination</code><code class="p">);</code>
  <code class="nx">audio</code><code class="p">.</code><code class="nx">src</code> <code class="o">=</code> <code class="err">'</code><a class="ulink" href="http://example.com/the.mp3" target="_top">http://example.com/the.mp3</a><code class="err">'</code><code class="p">;</code>
  <code class="nx">audio</code><code class="p">.</code><code class="nx">play</code><code class="p">();</code>
<code class="p">}</code></pre>
</div>
<div class="sect1" data-original-filename="ch07.xml" id="s07_2">
<div class="titlepage"><div><div><h2 class="title">Live Audio Input</h2></div></div></div>
<p id="one_highly_requ">One highly requested feature of the Web Audio API is integration
    with <code class="literal">getUserMedia</code>, which gives browsers
    access to the audio/video stream of connected microphones and cameras. At
    the time of this writing, this feature is available behind a flag in
    Chrome. To enable it, you need to visit <code class="literal">about:flags</code> and turn on the “Web Audio Input”
    experiment, as in <a class="xref" href="ch07.html#fig27" title="Figure 7-1. Enabling web audio input in Chrome">Figure 7-1</a>.</p>
<div class="figure" id="fig27">
<div class="figure-contents"><div class="mediaobject"><img src="http://orm-chimera-prod.s3.amazonaws.com/1234000001552/images/waap_0701.png" alt="Enabling web audio input in Chrome"></div></div>
<div class="figure-title">Figure 7-1. Enabling web audio input in Chrome</div>
</div>
<p id="once_this_is_en">Once this is enabled, you can use the <code class="literal">MediaStreamSourceNode</code> Web Audio node. This node
    wraps around the audio stream object that is available once the stream is
    established. This is directly analogous to the way that <code class="literal">MediaElementSourceNode</code>s wrap <code class="literal">&lt;audio&gt;</code> elements. In the following sample,
    we visualize the live audio input that has been processed by a notch
    filter:</p>
<pre class="programlisting" data-language="javascript" id="function_getliv"><code class="kd">function</code> <code class="nx">getLiveInput</code><code class="p">()</code> <code class="p">{</code>
  <code class="c1">// Only get the audio stream.</code>
  <code class="nx">navigator</code><code class="p">.</code><code class="nx">webkitGetUserMedia</code><code class="p">({</code><code class="nx">audio</code><code class="o">:</code> <code class="kc">true</code><code class="p">},</code> <code class="nx">onStream</code><code class="p">,</code> <code class="nx">onStreamError</code><code class="p">);</code>
<code class="p">};</code>

<code class="kd">function</code> <code class="nx">onStream</code><code class="p">(</code><code class="nx">stream</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Wrap a MediaStreamSourceNode around the live input stream.</code>
  <code class="kd">var</code> <code class="nx">input</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createMediaStreamSource</code><code class="p">(</code><code class="nx">stream</code><code class="p">);</code>
  <code class="c1">// Connect the input to a filter.</code>
  <code class="kd">var</code> <code class="nx">filter</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createBiquadFilter</code><code class="p">();</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">frequency</code><code class="p">.</code><code class="nx">value</code> <code class="o">=</code> <code class="mf">60.0</code><code class="p">;</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">type</code> <code class="o">=</code> <code class="nx">filter</code><code class="p">.</code><code class="nx">NOTCH</code><code class="p">;</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">Q</code> <code class="o">=</code> <code class="mf">10.0</code><code class="p">;</code>

  <code class="kd">var</code> <code class="nx">analyser</code> <code class="o">=</code> <code class="nx">context</code><code class="p">.</code><code class="nx">createAnalyser</code><code class="p">();</code>

  <code class="c1">// Connect graph.</code>
  <code class="nx">input</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">filter</code><code class="p">);</code>
  <code class="nx">filter</code><code class="p">.</code><code class="nx">connect</code><code class="p">(</code><code class="nx">analyser</code><code class="p">);</code>

  <code class="c1">// Set up an animation.</code>
  <code class="nx">requestAnimationFrame</code><code class="p">(</code><code class="nx">render</code><code class="p">);</code>
<code class="p">};</code>

<code class="kd">function</code> <code class="nx">onStreamError</code><code class="p">(</code><code class="nx">e</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">error</code><code class="p">(</code><code class="nx">e</code><code class="p">);</code>
<code class="p">};</code>

<code class="kd">function</code> <code class="nx">render</code><code class="p">()</code> <code class="p">{</code>
  <code class="c1">// Visualize the live audio input.</code>
  <code class="nx">requestAnimationFrame</code><code class="p">(</code><code class="nx">render</code><code class="p">);</code>
<code class="p">};</code></pre>
<p id="another_way_to_">Another way to establish streams is based on a WebRTC
    PeerConnection. By bringing a communication stream into the Web Audio API,
    you could, for example, spatialize multiple participants in a video
    conference.</p>
</div>
<div class="sect1" data-original-filename="ch07.xml" id="s07_3">
<div class="titlepage"><div><div><h2 class="title">Page Visibility and Audio Playback</h2></div></div></div>
<p id="whenever_you_de">Whenever you develop a web application that involves audio playback,
    you should be cognizant of the state of the page. The classic failure mode
    here is that one of many tabs is playing sound, but you have no idea which
    one it is. This may make sense for a music player application, in which
    you want music to continue playing regardless of the visibility of the
    page. However, for a game, you often want to pause gameplay (and sound
    playback) when the page is no longer in the foreground.</p>
<p id="luckily_the_pa">Luckily, the Page Visibility API provides functionality to detect
    when a page becomes hidden or visible. The state can be determined from
    the Boolean <code class="literal">document.hidden</code> property.
    The event that fires when the visibility changes is called <code class="literal">visibilitychange</code>. Because the API is still
    considered to be experimental, all of these names are webkit-prefixed.
    With this in mind, the following code will stop a source node when a page
    becomes hidden, and resume it when the page becomes visible:</p>
<pre class="programlisting" data-language="javascript" id="listen_to_th"><code class="c1">// Listen to the webkitvisibilitychange event.</code>
<code class="nb">document</code><code class="p">.</code><code class="nx">addEventListener</code><code class="p">(</code><code class="s1">'webkitvisibilitychange'</code><code class="p">,</code> <code class="nx">onVisibilityChange</code><code class="p">);</code>

<code class="kd">function</code> <code class="nx">onVisibilityChange</code><code class="p">()</code> <code class="p">{</code>
  <code class="k">if</code> <code class="p">(</code><code class="nb">document</code><code class="p">.</code><code class="nx">webkitHidden</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">source</code><code class="p">.</code><code class="nx">stop</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="p">}</code> <code class="k">else</code> <code class="p">{</code>
    <code class="nx">source</code><code class="p">.</code><code class="nx">start</code><code class="p">(</code><code class="mi">0</code><code class="p">);</code>
  <code class="p">}</code>
<code class="p">}</code></pre>
</div></section><section class="chapter" data-original-filename="ch08.xml" id="conclusion"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Conclusion</h1></div></div></div>
<p id="thanks_for_read">Thanks for reading this book on the Web Audio API. If you are a
  digital-audio novice, I hope that I have succeeded in giving you a solid
  understanding of some of the fundamental concepts. If you are a Web Audio
  API enthusiast, hopefully you learned something new.</p>
<p id="before_closing">Before closing, I would like to point you to a number of excellent
  books and web resources that I found extremely interesting and useful while
  researching and writing this book. My top five follow:</p>
<div class="orderedlist" id="the_web_audio__id1"><ol class="orderedlist" type="1">
<li class="listitem"><p id="the_web_audio__id2">The <a class="ulink" href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html" target="_top">“Web
      Audio API Specification”</a> by Chris Rogers</p></li>
<li class="listitem"><p id="real_sound_synt"><span class="emphasis"><em>Real Sound Synthesis for Interactive
      Applications</em></span> by Perry R. Cook (A K Peters, 2002)</p></li>
<li class="listitem"><p id="mastering_audio"><span class="emphasis"><em>Mastering Audio: The Art and the Science</em></span> by
      Bob Katz (Focal Press, 2002)</p></li>
<li class="listitem"><p id="andy_farnells_">Andy Farnell’s <a class="ulink" href="http://obiwannabe.co.uk/tutorials/html/tutorials_main.html" target="_top">“Practical
      Synthetic Sound Design”</a> tutorials</p></li>
<li class="listitem"><p id="all_about_deci"><a class="ulink" href="http://faculty.mccneb.edu/ccarlson/VACA1010/VACA1010_CD/dB%20part%201.pdf" target="_top">“All
      About Decibels, Part I: What’s your dB IQ?”</a> by Lionel
      Dumond</p></li>
</ol></div></section><section class="appendix" data-original-filename="appa.xml" id="app01"><div class="titlepage"><div><div><h1 class="title">Appendix A. Deprecation Notes</h1></div></div></div>
<p id="the_web_audio_a_id11">The Web Audio API is still evolving, and some methods are being added,
  removed, and renamed. This section describes some of the recent changes made
  to the API:</p>
<div class="itemizedlist" id="audiobuffersour_id1"><ul class="itemizedlist">
<li class="listitem"><p id="audiobuffersour_id2"><code class="literal">AudioBufferSourceNode.noteOn()</code>
      has been changed to <code class="literal">start()</code>.</p></li>
<li class="listitem"><p id="audiobuffersour_id3"><code class="literal">AudioBufferSourceNode.noteGrainOn()</code> has been
      changed to <code class="literal">start()</code>.</p></li>
<li class="listitem"><p id="audiobuffersour_id4"><code class="literal">AudioBufferSourceNode.noteOff()</code>
      has been changed to <code class="literal">stop()</code>.</p></li>
<li class="listitem"><p id="audiocontextcr_id1"><code class="literal">AudioContext.createGainNode()</code>
      has been changed to <code class="literal">createGain()</code>.</p></li>
<li class="listitem"><p id="audiocontextcr_id2"><code class="literal">AudioContext.createDelayNode()</code>
      has been changed to <code class="literal">createDelay()</code>.</p></li>
<li class="listitem"><p id="audiocontextcr_id3"><code class="literal">AudioContext.createJavaScriptNode()</code> has been
      changed to <code class="literal">createScriptProcessor()</code>.</p></li>
<li class="listitem"><p id="oscillatornode_id1"><code class="literal">OscillatorNode.noteOn()</code> has
      been changed to <code class="literal">start()</code>.</p></li>
<li class="listitem"><p id="oscillatornode_id2"><code class="literal">OscillatorNode.noteOff()</code> has
      been changed to <code class="literal">stop()</code>.</p></li>
<li class="listitem"><p id="audioparamsett"><code class="literal">AudioParam.setTargetValueAtTime()</code> has been
      changed to <code class="literal">setTargetAtTime()</code>.</p></li>
</ul></div>
<p id="in_addition_to__id5">In addition to these changes, many of the constants in the Web Audio
  API are changing from variables into string enumerations. For example,
  filter types are going from <code class="literal">filter.LOWPASS</code> to <code class="literal">lowpass</code>, oscillator types are going from <code class="literal">osc.SINE</code> to <code class="literal">sine</code>, etc.</p>
<p id="throughout_the_">Throughout the book, I’ve used new versions of all of the APIs, so
  those using older implementations of the API may need to revert back to
  earlier methods and constant naming.</p>
<p id="for_the_most_up">For the most up-to-date information regarding naming changes, see the
  <a class="ulink" href="http://bit.ly/105wGL0" target="_top">Web Audio specification</a>.</p></section><section class="appendix" data-original-filename="appb.xml" id="app02"><div class="titlepage"><div><div><h1 class="title">Appendix B. Glossary</h1></div></div></div>
<div class="variablelist" id="audio_contexta_"><dl class="variablelist">
<dt><span class="term">Audio context</span></dt>
<dd><p id="a_container_for">A container for all audio nodes in the Web Audio API
        graph.</p></dd>
<dt><span class="term">Bit depth</span></dt>
<dd><p id="the_number_of_b_id1">The number of bits allocated for each value in an audio data
        stream.</p></dd>
<dt><span class="term">Bit rate</span></dt>
<dd><p id="the_number_of_b_id2">The number of bits per second that a compressed audio file will
        output.</p></dd>
<dt><span class="term">Cents</span></dt>
<dd><p id="a_logarithmic_u">A logarithmic unit of measure used for musical intervals.
        Twelve-tone equal temperament divides the octave into 12 semitones of
        100 cents each.</p></dd>
<dt><span class="term">Clipping</span></dt>
<dd><p id="what_happens_to">What happens to an audio wave when it exceeds the highest
        permitted value (0 dBFS).</p></dd>
<dt><span class="term">Decibels</span></dt>
<dd><p id="a_relative_unit">A relative unit used to measure the intensity of a sound
        signal.</p></dd>
<dt><span class="term">dBFS</span></dt>
<dd><p id="sound_level_rel">Sound level relative to the full scale (nominally 0 dBFS). This
        unit is negative unless sound is being clipped.</p></dd>
<dt><span class="term">dBSPL</span></dt>
<dd><p id="sound_pressure_">Sound pressure level relative to the threshold of human
        hearing.</p></dd>
<dt><span class="term">FFT</span></dt>
<dd><p id="fast_fourier_tr">Fast Fourier Transform, an algorithm for breaking a sound wave
        up into its constituent sine waves.</p></dd>
<dt><span class="term">Hertz</span></dt>
<dd><p id="a_measure_of_fr">A measure of frequency. The number of times per second that
        something happens.</p></dd>
<dt><span class="term">Nyquist frequency</span></dt>
<dd><p id="half_of_the_sam">Half of the sample rate of an audio buffer.</p></dd>
<dt><span class="term">PCM</span></dt>
<dd><p id="pulse_code_modu">Pulse code modulation, a way of storing sound waves as an array
        of numbers.</p></dd>
<dt><span class="term">Playback rate</span></dt>
<dd><p id="the_speed_at_wh">The speed at which an audio buffer is played back.</p></dd>
<dt><span class="term">Sample rate</span></dt>
<dd><p id="the_number_of_t">The number of times per second that an analog sound is sampled
        during quantization, or the process of converting an analog signal
        into a digital one.</p></dd>
</dl></div></section><div class="colophon" id="colophon">
<h1 class="title" id="colophon">Colophon</h1>
<p id="the_animal_on_t">The animal on the cover of <span class="emphasis"><em>Web Audio API </em></span>is a
  brown long-eared bat (<span class="emphasis"><em>Plecotus auritus</em></span>).</p>
<p id="the_cover_image">The cover image is from Cassell’s <span class="emphasis"><em>Natural
  History</em></span>. The cover font is Adobe ITC Garamond. The text font is
  Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code
  font is Dalton Maag's Ubuntu Mono.</p>
</div>
